{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM4513] Assignment: Text Classification with a Feedforward Network\n",
    "\n",
    "\n",
    "## Assignment: Jooho Lee\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop a Feedforward network for text classification. \n",
    "\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for transforming raw text data into input vectors for your network  (**1 mark**)\n",
    "- A Feedforward network consisting of:\n",
    "    - **One-hot** input layer mapping words into an **Embedding weight matrix** (**1 mark**)\n",
    "    - **One hidden layer** computing the mean embedding vector of all words in input followed by a **ReLU activation function** (**1 mark**)\n",
    "    - **Output layer** with a **softmax** activation. (**1 mark**)\n",
    "- The Stochastic Gradient Descent (SGD) algorithm with **back-propagation** to learn the weights of your Neural network. Your algorithm should:\n",
    "    - Use (and minimise) the **Categorical Cross-entropy loss** function (**1 mark**)\n",
    "    - Perform a **Forward pass** to compute intermediate outputs (**4 marks**)\n",
    "    - Perform a **Backward pass** to compute gradients and update all sets of weights (**4 marks**)\n",
    "    - Implement and use **Dropout** after each hidden layer for regularisation (**2 marks**)\n",
    "- Discuss how did you choose hyperparameters? You can tune the learning rate (hint: choose small values), embedding size {e.g. 50, 300, 500} and the dropout rate {e.g. 0.2, 0.5}. Please use tables or graphs to show training and validation performance for each hyperparameter combination  (**2 marks**). \n",
    "- After training the model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot and report accuracy.\n",
    "- Re-train your network by using pre-trained embeddings ([GloVe](https://nlp.stanford.edu/projects/glove/)) trained on large corpora. Instead of randomly initialising the embedding weights matrix, you should initialise it with the pre-trained weights. During training, you should not update them (i.e. weight freezing) and backprop should stop before computing gradients for updating embedding weights. Report results by performing hyperparameter tuning and plotting the learning process. Do you get better performance? (**3 marks**).\n",
    "\n",
    "- **BONUS:** Extend you Feedforward network by adding more hidden layers (e.g. one more). How does it affect the performance? Note: You need to repeat hyperparameter tuning, but the number of combinations grows exponentially. Therefore, you need to choose a subset of all possible combinations (**+2 extra marks**)\n",
    "\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use for Task 2 is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "### Pre-trained Embeddings\n",
    "\n",
    "You can download pre-trained GloVe embeddings trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) from [here](http://nlp.stanford.edu/data/glove.840B.300d.zip). No need to unzip, the file is large.\n",
    "\n",
    "### Save Memory\n",
    "\n",
    "To save RAM, when you finish each experiment you can delete the weights of your network using `del W` followed by Python's garbage collector `gc.collect()`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given functions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc.. \n",
    "\n",
    "Please make sure to comment your code. You should also mention if you've used Windows to write and test your code. There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1 of roughly 75-80% and ~85% without and with using pre-trained embeddings respectively. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 30\\% of your final grade in the module. If you implement the bonus question you can get up to 2 extra points but your final grade will be capped at 20.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Thursday, 14 April 2022** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index), including Turnitin which helps detect plagiarism, so make sure you do not plagiarise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code run on windows 10, Python anaconda distribution 3.9.12\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set samples\n",
      "      label                                               data\n",
      "1348      2  US News, NEW YORK - A teenager from New York s...\n",
      "468       1  AP - Venezuelan election officials agreed Tues...\n",
      "1463      2  AP - Barry Bonds homered for the third time in...\n",
      "2267      3  SAN FRANCISCO -- A Colorado assistant store ma...\n",
      "943       2   ATHENS (Reuters) - Australian swimmer Ian Tho...\n",
      "Test set samples\n",
      "     label                                               data\n",
      "563      2  NBC and its family of cable networks flooded A...\n",
      "552      2  A defrocked Irish priest who attacked the lead...\n",
      "439      2  Tony Dickens resigns as head coach at Northwes...\n",
      "169      1  Gongzhong does not resemble any Tibetan villag...\n",
      "152      1  AP - Lawyers pressed Chile's Supreme Court on ...\n",
      "Development set samples\n",
      "    label                                               data\n",
      "47      1   DUBAI/PARIS (Reuters) - The French government...\n",
      "3       1  The leader of militant Lebanese group Hezbolla...\n",
      "31      1  NEW DELHI, 7 September 2004 - India and Pakist...\n",
      "25      1  Reuters - A statement posted on a Web site\\pur...\n",
      "15      1  File photo taken on March 1, 2003 shows Izzat ...\n"
     ]
    }
   ],
   "source": [
    "# these CSV files have no header, just in the format class / string\n",
    "train_set = pd.read_csv('data_topic/train.csv', names=['label','data']) \n",
    "test_set = pd.read_csv('data_topic/test.csv', names=['label','data'])\n",
    "dev_set = pd.read_csv('data_topic/dev.csv', names=['label','data'])\n",
    "\n",
    "# Print some samples for each set\n",
    "\n",
    "print(\"Training set samples\")\n",
    "print(train_set.sample(n=5, random_state=1))\n",
    "\n",
    "print(\"Test set samples\")\n",
    "print(test_set.sample(n=5, random_state=2))\n",
    "\n",
    "print(\"Development set samples\")\n",
    "print(dev_set.sample(n=5, random_state=3))\n",
    "\n",
    "full_corpus = pd.concat([train_set, test_set, dev_set]) # Full corpus for unigram extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams (tip: you can re-use the functions from Assignment 1) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:40.851926Z",
     "start_time": "2020-04-02T14:26:40.847500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its',\n",
    "             '-', '--', '#39;s', '...'] # This row was added manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:23:17.181553Z",
     "start_time": "2020-05-11T08:23:17.178314Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'hello', 'good', 'to', 'see', 'you', 'hello', 'nice', 'to', 'meet', 'you', ('hello', 'world', 'hello'), ('world', 'hello', 'good'), ('hello', 'good', 'to'), ('good', 'to', 'see'), ('to', 'see', 'you'), ('see', 'you', 'hello'), ('you', 'hello', 'nice'), ('hello', 'nice', 'to'), ('nice', 'to', 'meet'), ('to', 'meet', 'you')]\n",
      "['hello', 'hello', 'hello', 'nice', ('hello', 'good', 'to')]\n"
     ]
    }
   ],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), \n",
    "                   token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "                   stop_words=[], \n",
    "                   vocab=set()):\n",
    "    words = re.findall(token_pattern,x_raw.lower()) # Split with regex pattern\n",
    "    words = [word for word in words if not word in stop_words] # remove all words in the stop_words list\n",
    "    \n",
    "    x = [] # Full output list\n",
    "    for n in ngram_range:\n",
    "        ngram_ftrs = []\n",
    "        for i in range(len(words)): \n",
    "            if i+n <= len(words):\n",
    "                ngram = words[i:i+n] # Iterate over the whole list and select the n words in succession\n",
    "                if n == 1:\n",
    "                    ngram = ngram[0] # Just get the string if it's an unigram\n",
    "                else:\n",
    "                    ngram = tuple(ngram) # If not it should be a tuple of strings (for keying later)\n",
    "                \n",
    "                ngram_ftrs.append(ngram)\n",
    "        x.extend(ngram_ftrs)\n",
    "    if len(vocab) > 0: # Filter the output to only be specific features\n",
    "        x = [x_vocab for x_vocab in x if x_vocab in vocab]\n",
    "    return x\n",
    "\n",
    "# Testing:\n",
    "print(extract_ngrams(\"Hello world Hello good to see you hello nice to meet you\", ngram_range=(1,3)))\n",
    "print(extract_ngrams(\"Hello world Hello good to see you hello nice to meet you\", ngram_range=(1,3), vocab=[\"hello\",\"nice\",(\"hello\", \"good\", \"to\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "              min_df=0, keep_topN=0, stop_words=[]):\n",
    "    full_list = [] # List with ALL ngrams \n",
    "    vocab = []\n",
    "    ngram_counts = []\n",
    "    full_counts = []\n",
    "    for raw_text in X_raw:\n",
    "        extracted = extract_ngrams(raw_text, ngram_range=ngram_range, token_pattern=token_pattern, stop_words=stop_words) # Use the defined function to extract all features\n",
    "        full_list.extend(extracted)\n",
    "    counts=Counter(full_list) \n",
    "    sorted_counts = counts.most_common()\n",
    "    full_counts.append(sorted_counts)\n",
    "    # Use min_df to drop all features with less than min_df\n",
    "    vocabulary_dict = { k:v for k,v in sorted_counts if v >= min_df }\n",
    "\n",
    "    # Select top keep_topN ngrams if defined\n",
    "    if keep_topN > 0:\n",
    "        vocabulary_dict = {k: vocabulary_dict[k] for k in list(vocabulary_dict.keys())[:keep_topN]}\n",
    "\n",
    "    vocab = list(vocabulary_dict.keys())\n",
    "    return vocab, sorted_counts, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corp_vocabs, corp_dfs, corp_counts = get_vocab(full_corpus['data'].tolist(), ngram_range=[1], stop_words=stop_words, min_df=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and word -> vocabulary id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:44.069661Z",
     "start_time": "2020-04-02T14:26:44.065058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The lookup dicts for id -> word and word -> id\n",
    "\n",
    "def vocab_id_to_word(vocab, raw_ids):\n",
    "    map_dict = {i: vocab[i] for i in range(len(vocab))}\n",
    "    out = [map_dict[idx] for idx in raw_ids]\n",
    "    return out\n",
    "\n",
    "def vocab_word_to_id(vocab, raw_text):\n",
    "    map_dict = {vocab[i]:i for i in range(len(vocab))}\n",
    "    out = [map_dict[w] for w in raw_text]\n",
    "    return out  \n",
    "\n",
    "def vocab_id_to_word_list(vocab, raw_ids_list):\n",
    "    out = []\n",
    "    for raw_ids in raw_ids_list:\n",
    "        out.append(vocab_id_to_word(vocab, raw_ids))\n",
    "    return out\n",
    "\n",
    "def vocab_word_to_id_list(vocab, raw_text_list):\n",
    "    out = []\n",
    "    for raw_text in raw_text_list:\n",
    "        out.append(vocab_word_to_id(vocab, raw_text))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "First, represent documents in train, dev and test sets as lists of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.047887Z",
     "start_time": "2020-04-02T14:26:44.920631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_ngrams_list(X_raw, ngram_range=(1,3), \n",
    "                   token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "                   stop_words=[], \n",
    "                   vocab=set()):\n",
    "    out = []\n",
    "    for raw_text in X_raw:\n",
    "        extracted = extract_ngrams(raw_text, ngram_range=ngram_range, token_pattern=token_pattern, stop_words=stop_words, vocab=vocab) # Use the defined function to extract all features\n",
    "        out.append(extracted)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "X_tr_raw = train_set['data'].tolist()\n",
    "X_dev_raw = dev_set['data'].tolist()\n",
    "X_test_raw = test_set['data'].tolist()\n",
    "\n",
    "\n",
    "X_uni_tr = extract_ngrams_list(X_tr_raw, ngram_range=[1], stop_words=stop_words, vocab=corp_vocabs)\n",
    "X_uni_dev = extract_ngrams_list(X_dev_raw, ngram_range=[1], stop_words=stop_words, vocab=corp_vocabs)\n",
    "X_uni_test = extract_ngrams_list(X_test_raw, ngram_range=[1], stop_words=stop_words, vocab=corp_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.219580Z",
     "start_time": "2020-04-02T14:26:45.214630Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuters',\n",
       " 'venezuelans',\n",
       " 'turned',\n",
       " 'out',\n",
       " 'early',\n",
       " 'large',\n",
       " 'numbers',\n",
       " 'sunday',\n",
       " 'vote',\n",
       " 'historic',\n",
       " 'referendum',\n",
       " 'either',\n",
       " 'remove',\n",
       " 'left',\n",
       " 'wing',\n",
       " 'president',\n",
       " 'hugo',\n",
       " 'chavez',\n",
       " 'office',\n",
       " 'give',\n",
       " 'him',\n",
       " 'new',\n",
       " 'mandate',\n",
       " 'govern',\n",
       " 'next',\n",
       " 'two',\n",
       " 'years']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_uni_tr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert them into lists of indices in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.752658Z",
     "start_time": "2020-04-02T14:26:45.730409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tr = vocab_word_to_id_list(corp_vocabs, X_uni_tr)\n",
    "X_dev = vocab_word_to_id_list(corp_vocabs, X_uni_dev)\n",
    "X_test = vocab_word_to_id_list(corp_vocabs, X_uni_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:46.203894Z",
     "start_time": "2020-04-02T14:26:46.199850Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1197,\n",
       " 844,\n",
       " 31,\n",
       " 210,\n",
       " 1038,\n",
       " 1303,\n",
       " 21,\n",
       " 370,\n",
       " 790,\n",
       " 384,\n",
       " 1708,\n",
       " 2949,\n",
       " 134,\n",
       " 959,\n",
       " 35,\n",
       " 233,\n",
       " 225,\n",
       " 430,\n",
       " 613,\n",
       " 112,\n",
       " 2,\n",
       " 1709,\n",
       " 2950,\n",
       " 155,\n",
       " 11,\n",
       " 64]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuters',\n",
       " 'said',\n",
       " 'new',\n",
       " 'tuesday',\n",
       " 'after',\n",
       " 'wednesday',\n",
       " 'athens',\n",
       " 'ap',\n",
       " 'monday',\n",
       " 'us',\n",
       " 'first',\n",
       " 'two',\n",
       " 'over',\n",
       " 'york',\n",
       " 'olympic',\n",
       " 'oil',\n",
       " 'inc',\n",
       " 'more',\n",
       " 'year',\n",
       " 'company',\n",
       " 'world',\n",
       " 'sunday',\n",
       " 'prices',\n",
       " 'one',\n",
       " 'gt',\n",
       " 'lt',\n",
       " 'quot',\n",
       " 'had',\n",
       " 'second',\n",
       " 'than',\n",
       " 'united',\n",
       " 'out',\n",
       " 'up',\n",
       " 'about',\n",
       " 'against',\n",
       " 'president',\n",
       " 'last',\n",
       " 'iraq',\n",
       " 'three',\n",
       " 'into',\n",
       " 'aug',\n",
       " 'yesterday',\n",
       " 'fullquote',\n",
       " 'night',\n",
       " 'when',\n",
       " 'gold',\n",
       " 'stocks',\n",
       " 'team',\n",
       " 'time',\n",
       " 'day',\n",
       " 'states',\n",
       " 'percent',\n",
       " 'week',\n",
       " 'off',\n",
       " 'win',\n",
       " 'greece',\n",
       " 'olympics',\n",
       " 'games',\n",
       " 'would',\n",
       " 'washington',\n",
       " 'home',\n",
       " 'government',\n",
       " 'men',\n",
       " 'medal',\n",
       " 'years',\n",
       " 'city',\n",
       " 'american',\n",
       " 'record',\n",
       " 'won',\n",
       " 'four',\n",
       " 'million',\n",
       " 'all',\n",
       " 'people',\n",
       " 'billion',\n",
       " 'minister',\n",
       " 'back',\n",
       " 'afp',\n",
       " 'officials',\n",
       " 'najaf',\n",
       " 'today',\n",
       " 'public',\n",
       " 'final',\n",
       " 'country',\n",
       " 'al',\n",
       " 'google',\n",
       " 'state',\n",
       " 'month',\n",
       " 'end',\n",
       " 'com',\n",
       " 'national',\n",
       " 'top',\n",
       " 'shares',\n",
       " 'investor',\n",
       " 'group',\n",
       " 'corp',\n",
       " 'china',\n",
       " 'game',\n",
       " 'profit',\n",
       " 'ticker',\n",
       " 'most',\n",
       " 'before',\n",
       " 'bank',\n",
       " 'target',\n",
       " 'former',\n",
       " 'international',\n",
       " 'plans',\n",
       " 'reported',\n",
       " 'sales',\n",
       " 'high',\n",
       " 'thursday',\n",
       " 'run',\n",
       " 'season',\n",
       " 'him',\n",
       " 'friday',\n",
       " 'iraqi',\n",
       " 'london',\n",
       " 'price',\n",
       " 'john',\n",
       " 'third',\n",
       " 'offering',\n",
       " 'http',\n",
       " 'victory',\n",
       " 'largest',\n",
       " 'href',\n",
       " 'www',\n",
       " 'aspx',\n",
       " 'quickinfo',\n",
       " 'just',\n",
       " 'five',\n",
       " 'troops',\n",
       " 'hit',\n",
       " 'michael',\n",
       " 'court',\n",
       " 'quarter',\n",
       " 'left',\n",
       " 'between',\n",
       " 'down',\n",
       " 'some',\n",
       " 'stock',\n",
       " 'other',\n",
       " 'round',\n",
       " 'initial',\n",
       " 'least',\n",
       " 'prime',\n",
       " 'expected',\n",
       " 'women',\n",
       " 'eight',\n",
       " 'another',\n",
       " 'during',\n",
       " 'may',\n",
       " 'market',\n",
       " 'south',\n",
       " 'while',\n",
       " 'made',\n",
       " 'investors',\n",
       " 'next',\n",
       " 'police',\n",
       " 'set',\n",
       " 'higher',\n",
       " 'chicago',\n",
       " 'rose',\n",
       " 'israeli',\n",
       " 'since',\n",
       " 'major',\n",
       " 'federal',\n",
       " 'days',\n",
       " 'co',\n",
       " 'australia',\n",
       " 'bush',\n",
       " 'british',\n",
       " 'england',\n",
       " 'july',\n",
       " 'leader',\n",
       " 'phelps',\n",
       " 'crude',\n",
       " 'earnings',\n",
       " 'cut',\n",
       " 'economy',\n",
       " 'fell',\n",
       " 'champion',\n",
       " 'took',\n",
       " 'talks',\n",
       " 'biggest',\n",
       " 'san',\n",
       " 'saturday',\n",
       " 'under',\n",
       " 'announced',\n",
       " 'costs',\n",
       " 'lead',\n",
       " 'half',\n",
       " 'according',\n",
       " 'news',\n",
       " 'league',\n",
       " 'plan',\n",
       " 'baghdad',\n",
       " 'war',\n",
       " 'west',\n",
       " 'business',\n",
       " 'financial',\n",
       " 'hurricane',\n",
       " 'because',\n",
       " 'nearly',\n",
       " 'report',\n",
       " 'cleric',\n",
       " 'killed',\n",
       " 'seven',\n",
       " 'sadr',\n",
       " 'paul',\n",
       " 'through',\n",
       " 'near',\n",
       " 'early',\n",
       " 'military',\n",
       " 'where',\n",
       " 'basketball',\n",
       " 'giant',\n",
       " 'says',\n",
       " 'canadian',\n",
       " 'fourth',\n",
       " 'official',\n",
       " 'street',\n",
       " 'six',\n",
       " 'consumer',\n",
       " 'quarterly',\n",
       " 'race',\n",
       " 'long',\n",
       " 'chavez',\n",
       " 'election',\n",
       " 'fighting',\n",
       " 'only',\n",
       " 'shot',\n",
       " 'ahead',\n",
       " 'sox',\n",
       " 'per',\n",
       " 'hugo',\n",
       " 'old',\n",
       " 'held',\n",
       " 'buy',\n",
       " 'strong',\n",
       " 'right',\n",
       " 'even',\n",
       " 'loss',\n",
       " 'economic',\n",
       " 'capital',\n",
       " 'start',\n",
       " 'agreed',\n",
       " 'wall',\n",
       " 'players',\n",
       " 'months',\n",
       " 'put',\n",
       " 'region',\n",
       " 'help',\n",
       " 'man',\n",
       " 'go',\n",
       " 'mark',\n",
       " 'still',\n",
       " 'winning',\n",
       " 'coach',\n",
       " 'research',\n",
       " 'dollar',\n",
       " 'share',\n",
       " 'security',\n",
       " 'here',\n",
       " 'service',\n",
       " 'nation',\n",
       " 'forces',\n",
       " 'meter',\n",
       " 'open',\n",
       " 'cup',\n",
       " 'north',\n",
       " 'say',\n",
       " 'now',\n",
       " 'big',\n",
       " 'securities',\n",
       " 'based',\n",
       " 'maker',\n",
       " 'venezuela',\n",
       " 'being',\n",
       " 'earlier',\n",
       " 'search',\n",
       " 'beat',\n",
       " 'make',\n",
       " 'close',\n",
       " 'foreign',\n",
       " 'including',\n",
       " 'charley',\n",
       " 'freestyle',\n",
       " 'russian',\n",
       " 'august',\n",
       " 'army',\n",
       " 'peace',\n",
       " 'holy',\n",
       " 'press',\n",
       " 'exchange',\n",
       " 'gaza',\n",
       " 'posted',\n",
       " 'profile',\n",
       " 'tokyo',\n",
       " 'whether',\n",
       " 'medals',\n",
       " 'party',\n",
       " 'energy',\n",
       " 'commission',\n",
       " 'amid',\n",
       " 'militia',\n",
       " 'lost',\n",
       " 'leading',\n",
       " 'competition',\n",
       " 'nations',\n",
       " 'trading',\n",
       " 'data',\n",
       " 'western',\n",
       " 'general',\n",
       " 'results',\n",
       " 'many',\n",
       " 'department',\n",
       " 'florida',\n",
       " 'japan',\n",
       " 'way',\n",
       " 'past',\n",
       " 'trial',\n",
       " 'demand',\n",
       " 'much',\n",
       " 'quote',\n",
       " 'sudan',\n",
       " 'afghanistan',\n",
       " 'play',\n",
       " 'homes',\n",
       " 'britain',\n",
       " 'get',\n",
       " 'boston',\n",
       " 'deal',\n",
       " 'darfur',\n",
       " 'kerry',\n",
       " 'french',\n",
       " 'sports',\n",
       " 'ago',\n",
       " 'led',\n",
       " 'americans',\n",
       " 'part',\n",
       " 'services',\n",
       " 'francisco',\n",
       " 'europe',\n",
       " 'political',\n",
       " 'championship',\n",
       " 'beijing',\n",
       " 'european',\n",
       " 'case',\n",
       " 'bloomberg',\n",
       " 'lower',\n",
       " 'trade',\n",
       " 'inning',\n",
       " 'retailer',\n",
       " 'despite',\n",
       " 'history',\n",
       " 'again',\n",
       " 'around',\n",
       " 'conference',\n",
       " 'double',\n",
       " 'weeks',\n",
       " 'latest',\n",
       " 'drop',\n",
       " 'companies',\n",
       " 'greek',\n",
       " 'take',\n",
       " 'firm',\n",
       " 'tournament',\n",
       " 'chief',\n",
       " 'red',\n",
       " 'title',\n",
       " 'network',\n",
       " 'vote',\n",
       " 'killing',\n",
       " 'behind',\n",
       " 'radical',\n",
       " 'industry',\n",
       " 'russia',\n",
       " 'released',\n",
       " 'australian',\n",
       " 'taking',\n",
       " 'toronto',\n",
       " 'head',\n",
       " 'summer',\n",
       " 'helped',\n",
       " 'how',\n",
       " 'referendum',\n",
       " 'israel',\n",
       " 'korea',\n",
       " 'key',\n",
       " 'reports',\n",
       " 'like',\n",
       " 'murder',\n",
       " 'field',\n",
       " 'shiite',\n",
       " 'following',\n",
       " 'inflation',\n",
       " 'shi',\n",
       " 'showed',\n",
       " 'food',\n",
       " 'web',\n",
       " 'contract',\n",
       " 'decision',\n",
       " 'thousands',\n",
       " 'pay',\n",
       " 'air',\n",
       " 'should',\n",
       " 'authorities',\n",
       " 'charged',\n",
       " 'ever',\n",
       " 'interest',\n",
       " 'white',\n",
       " 'growth',\n",
       " 'defense',\n",
       " 'jones',\n",
       " 'union',\n",
       " 'ite',\n",
       " 'workers',\n",
       " 'sell',\n",
       " 'place',\n",
       " 'attack',\n",
       " 'opposition',\n",
       " 'accused',\n",
       " 'missed',\n",
       " 'bid',\n",
       " 'pakistan',\n",
       " 'late',\n",
       " 'player',\n",
       " 'estimates',\n",
       " 'career',\n",
       " 'labor',\n",
       " 'hamm',\n",
       " 'office',\n",
       " 'leaders',\n",
       " 'also',\n",
       " 'good',\n",
       " 'canada',\n",
       " 'committee',\n",
       " 'money',\n",
       " 'manager',\n",
       " 'drugs',\n",
       " 'cash',\n",
       " 'morning',\n",
       " 'little',\n",
       " 'germany',\n",
       " 'number',\n",
       " 'blue',\n",
       " 'tennis',\n",
       " 'palestinian',\n",
       " 'democratic',\n",
       " 'drug',\n",
       " 'ian',\n",
       " 'any',\n",
       " 'cost',\n",
       " 'republican',\n",
       " 'america',\n",
       " 'global',\n",
       " 'kenteris',\n",
       " 'face',\n",
       " 'atlanta',\n",
       " 'soldiers',\n",
       " 'india',\n",
       " 'called',\n",
       " 'running',\n",
       " 'across',\n",
       " 'shrine',\n",
       " 'internet',\n",
       " 'ended',\n",
       " 'sprinters',\n",
       " 'senior',\n",
       " 'silver',\n",
       " 'net',\n",
       " 'rebels',\n",
       " 'due',\n",
       " 'both',\n",
       " 'hours',\n",
       " 'found',\n",
       " 'france',\n",
       " 'match',\n",
       " 'southern',\n",
       " 'best',\n",
       " 'militants',\n",
       " 'los',\n",
       " 'charges',\n",
       " 'away',\n",
       " 'got',\n",
       " 'agreement',\n",
       " 'event',\n",
       " 'low',\n",
       " 'range',\n",
       " 'credit',\n",
       " 'nuclear',\n",
       " 'meeting',\n",
       " 'japanese',\n",
       " 'thorpe',\n",
       " 'release',\n",
       " 'recall',\n",
       " 'singh',\n",
       " 'angeles',\n",
       " 'making',\n",
       " 'same',\n",
       " 'texas',\n",
       " 'regulators',\n",
       " 'bill',\n",
       " 'continued',\n",
       " 'runs',\n",
       " 'stadium',\n",
       " 'barrel',\n",
       " 'amp',\n",
       " 'used',\n",
       " 'central',\n",
       " 'fire',\n",
       " 'school',\n",
       " 'agency',\n",
       " 'saying',\n",
       " 'center',\n",
       " 'keep',\n",
       " 'rates',\n",
       " 'burundi',\n",
       " 'heavy',\n",
       " 'such',\n",
       " 'straight',\n",
       " 'told',\n",
       " 'might',\n",
       " 'become',\n",
       " 'rebel',\n",
       " 'future',\n",
       " 'card',\n",
       " 'meters',\n",
       " 'innings',\n",
       " 'funds',\n",
       " 'fund',\n",
       " 'power',\n",
       " 'failed',\n",
       " 'rise',\n",
       " 'thanou',\n",
       " 'went',\n",
       " 'children',\n",
       " 'return',\n",
       " 'private',\n",
       " 'injury',\n",
       " 'fifth',\n",
       " 'recent',\n",
       " 'football',\n",
       " 'hospital',\n",
       " 'housing',\n",
       " 'pulled',\n",
       " 'selling',\n",
       " 'gymnastics',\n",
       " 'store',\n",
       " 'bomb',\n",
       " 'presidential',\n",
       " 'move',\n",
       " 'rival',\n",
       " 'came',\n",
       " 'relay',\n",
       " 'later',\n",
       " 'camp',\n",
       " 'convention',\n",
       " 'storm',\n",
       " 'hundreds',\n",
       " 'test',\n",
       " 'try',\n",
       " 'islamic',\n",
       " 'life',\n",
       " 'pool',\n",
       " 'captain',\n",
       " 'sharply',\n",
       " 'series',\n",
       " 'claims',\n",
       " 'filed',\n",
       " 'executive',\n",
       " 'work',\n",
       " 'september',\n",
       " 'republic',\n",
       " 'individual',\n",
       " 'far',\n",
       " 'williams',\n",
       " 'johnson',\n",
       " 'airlines',\n",
       " 'water',\n",
       " 'visit',\n",
       " 'sydney',\n",
       " 'among',\n",
       " 'short',\n",
       " 'continue',\n",
       " 'fresh',\n",
       " 'crisis',\n",
       " 'nine',\n",
       " 'moqtada',\n",
       " 'full',\n",
       " 'asked',\n",
       " 'starting',\n",
       " 'insurance',\n",
       " 'st',\n",
       " 'reserve',\n",
       " 'level',\n",
       " 'pressure',\n",
       " 'likely',\n",
       " 'drove',\n",
       " 'outlook',\n",
       " 'products',\n",
       " 'terror',\n",
       " 'star',\n",
       " 'well',\n",
       " 'less',\n",
       " 'gains',\n",
       " 'fall',\n",
       " 'real',\n",
       " 'kong',\n",
       " 'xinhuanet',\n",
       " 'roger',\n",
       " 'andy',\n",
       " 'soccer',\n",
       " 'engine',\n",
       " 'give',\n",
       " 'began',\n",
       " 'death',\n",
       " 'possible',\n",
       " 'caracas',\n",
       " 'finally',\n",
       " 'residents',\n",
       " 'opening',\n",
       " 'holding',\n",
       " 'going',\n",
       " 'weekend',\n",
       " 'soaring',\n",
       " 'great',\n",
       " 'vijay',\n",
       " 'anti',\n",
       " 'became',\n",
       " 'boost',\n",
       " 'construction',\n",
       " 'materials',\n",
       " 'june',\n",
       " 'yukos',\n",
       " 'suspects',\n",
       " 'hong',\n",
       " 'slashed',\n",
       " 'investigation',\n",
       " 'beating',\n",
       " 'baseball',\n",
       " 'ltd',\n",
       " 'didn',\n",
       " 'militiamen',\n",
       " 'efforts',\n",
       " 'georgia',\n",
       " 'kabul',\n",
       " 'free',\n",
       " 'hour',\n",
       " 'worries',\n",
       " 'calif',\n",
       " 'jerusalem',\n",
       " 'dropped',\n",
       " 'sent',\n",
       " 'delegation',\n",
       " 'rights',\n",
       " 'fears',\n",
       " 'appeal',\n",
       " 'association',\n",
       " 'action',\n",
       " 'approval',\n",
       " 'host',\n",
       " 'phone',\n",
       " 'almost',\n",
       " 'investment',\n",
       " 'ninth',\n",
       " 'euro',\n",
       " 'david',\n",
       " 'olympia',\n",
       " 'golf',\n",
       " 'performance',\n",
       " 'mutual',\n",
       " 'injured',\n",
       " 'campaign',\n",
       " 'toward',\n",
       " 'term',\n",
       " 'venezuelan',\n",
       " 'then',\n",
       " 'terrorism',\n",
       " 'town',\n",
       " 'members',\n",
       " 'urged',\n",
       " 'television',\n",
       " 'opened',\n",
       " 'begin',\n",
       " 'break',\n",
       " 'annual',\n",
       " 'trying',\n",
       " 'dow',\n",
       " 'tour',\n",
       " 'line',\n",
       " 'miss',\n",
       " 'leave',\n",
       " 'statement',\n",
       " 'sources',\n",
       " 'tony',\n",
       " 'italian',\n",
       " 'palestinians',\n",
       " 'club',\n",
       " 'gave',\n",
       " 'giants',\n",
       " 'rate',\n",
       " 'halliburton',\n",
       " 'ossetia',\n",
       " 'violence',\n",
       " 'un',\n",
       " 'getting',\n",
       " 'look',\n",
       " 'reach',\n",
       " 'rivals',\n",
       " 'van',\n",
       " 'battle',\n",
       " 'intelligence',\n",
       " 'province',\n",
       " 'chinese',\n",
       " 'asia',\n",
       " 'afghan',\n",
       " 'kostas',\n",
       " 'rule',\n",
       " 'appeared',\n",
       " 'fraud',\n",
       " 'died',\n",
       " 'rising',\n",
       " 'operator',\n",
       " 'tax',\n",
       " 'conflict',\n",
       " 'terrorist',\n",
       " 'markets',\n",
       " 'sharon',\n",
       " 'eased',\n",
       " 'road',\n",
       " 'baltimore',\n",
       " 'alleged',\n",
       " 'mike',\n",
       " 'houston',\n",
       " 'bay',\n",
       " 'tests',\n",
       " 'too',\n",
       " 'need',\n",
       " 'potential',\n",
       " 'forecast',\n",
       " 'yankees',\n",
       " 'doping',\n",
       " 'philadelphia',\n",
       " 'awaited',\n",
       " 'hd',\n",
       " 'airways',\n",
       " 'debt',\n",
       " 'send',\n",
       " 'strike',\n",
       " 'force',\n",
       " 'african',\n",
       " 'show',\n",
       " 'avoid',\n",
       " 'few',\n",
       " 'family',\n",
       " 'given',\n",
       " 'caused',\n",
       " 'grand',\n",
       " 'building',\n",
       " 'increase',\n",
       " 'katerina',\n",
       " 'already',\n",
       " 'operations',\n",
       " 'teams',\n",
       " 'wanted',\n",
       " 'health',\n",
       " 'pga',\n",
       " 'stores',\n",
       " 'times',\n",
       " 'bronze',\n",
       " 'ryder',\n",
       " 'raised',\n",
       " 'chain',\n",
       " 'management',\n",
       " 'growing',\n",
       " 'nfl',\n",
       " 'ariel',\n",
       " 'backed',\n",
       " 'green',\n",
       " 'east',\n",
       " 'showing',\n",
       " 'airline',\n",
       " 'federer',\n",
       " 'defensive',\n",
       " 'highs',\n",
       " 'hold',\n",
       " 'yet',\n",
       " 'looking',\n",
       " 'homer',\n",
       " 'check',\n",
       " 'historic',\n",
       " 'georgian',\n",
       " 'sudanese',\n",
       " 'democracy',\n",
       " 'pushed',\n",
       " 'fans',\n",
       " 'giving',\n",
       " 'course',\n",
       " 'sixth',\n",
       " 'qualifying',\n",
       " 'outside',\n",
       " 'house',\n",
       " 'jobs',\n",
       " 'support',\n",
       " 'administration',\n",
       " 'until',\n",
       " 'refugees',\n",
       " 'warned',\n",
       " 'popular',\n",
       " 'missing',\n",
       " 'attacks',\n",
       " 'car',\n",
       " 'own',\n",
       " 'without',\n",
       " 'program',\n",
       " 'uprising',\n",
       " 'local',\n",
       " 'surged',\n",
       " 'ancient',\n",
       " 'media',\n",
       " 'sharp',\n",
       " 'boosted',\n",
       " 'pitched',\n",
       " 'coming',\n",
       " 'track',\n",
       " 'profits',\n",
       " 'join',\n",
       " 'advanced',\n",
       " 'singapore',\n",
       " 'decline',\n",
       " 'cardinals',\n",
       " 'jump',\n",
       " 'moscow',\n",
       " 'confidence',\n",
       " 'consecutive',\n",
       " 'cleveland',\n",
       " 'defending',\n",
       " 'job',\n",
       " 'your',\n",
       " 'scored',\n",
       " 'system',\n",
       " 'ipo',\n",
       " 'exports',\n",
       " 'depot',\n",
       " 'turned',\n",
       " 'better',\n",
       " 'edwards',\n",
       " 'changes',\n",
       " 'fired',\n",
       " 'clashes',\n",
       " 'swimming',\n",
       " 'judge',\n",
       " 'analysts',\n",
       " 'huge',\n",
       " 'evening',\n",
       " 'cp',\n",
       " 'within',\n",
       " 'others',\n",
       " 'rest',\n",
       " 'though',\n",
       " 'impact',\n",
       " 'negotiations',\n",
       " 'survived',\n",
       " 're',\n",
       " 'chance',\n",
       " 'step',\n",
       " 'decided',\n",
       " 'rules',\n",
       " 'muqtada',\n",
       " 'supplies',\n",
       " 'points',\n",
       " 'northern',\n",
       " 'see',\n",
       " 'sept',\n",
       " 'meet',\n",
       " 'religious',\n",
       " 'arrested',\n",
       " 'heart',\n",
       " 'jewish',\n",
       " 'settlement',\n",
       " 'approved',\n",
       " 'moved',\n",
       " 'chip',\n",
       " 'process',\n",
       " 'sign',\n",
       " 'gasoline',\n",
       " 'computer',\n",
       " 'come',\n",
       " 'wife',\n",
       " 'losing',\n",
       " 'blair',\n",
       " 'concerns',\n",
       " 'sold',\n",
       " 'survey',\n",
       " 'indians',\n",
       " 'hits',\n",
       " 'brown',\n",
       " 'ratings',\n",
       " 'spending',\n",
       " 'software',\n",
       " 'equipment',\n",
       " 'stake',\n",
       " 'operating',\n",
       " 'nortel',\n",
       " 'mortgage',\n",
       " 'goog',\n",
       " 'prisoners',\n",
       " 'arab',\n",
       " 'rally',\n",
       " 'george',\n",
       " 'law',\n",
       " 'mass',\n",
       " 'iran',\n",
       " 'concern',\n",
       " 'fla',\n",
       " 'cause',\n",
       " 'insurers',\n",
       " 'fight',\n",
       " 'berlusconi',\n",
       " 'allen',\n",
       " 'congolese',\n",
       " 'arrived',\n",
       " 'along',\n",
       " 'cbs',\n",
       " 'gas',\n",
       " 'assault',\n",
       " 'call',\n",
       " 'uk',\n",
       " 'ministry',\n",
       " 'firms',\n",
       " 'march',\n",
       " 'nepal',\n",
       " 'foot',\n",
       " 'voted',\n",
       " 'cincinnati',\n",
       " 'claim',\n",
       " 'island',\n",
       " 'human',\n",
       " 'index',\n",
       " 'athletes',\n",
       " 'post',\n",
       " 'cuts',\n",
       " 'roddick',\n",
       " 'attempt',\n",
       " 'point',\n",
       " 'grew',\n",
       " 'offer',\n",
       " 'finished',\n",
       " 'equity',\n",
       " 'homered',\n",
       " 'above',\n",
       " 'champions',\n",
       " 'orioles',\n",
       " 'debut',\n",
       " 'montreal',\n",
       " 'jumped',\n",
       " 'technology',\n",
       " 'accounting',\n",
       " 'networks',\n",
       " 'wing',\n",
       " 'korean',\n",
       " 'wounding',\n",
       " 'armed',\n",
       " 'dead',\n",
       " 'ii',\n",
       " 'asian',\n",
       " 'remain',\n",
       " 'pope',\n",
       " 'hurt',\n",
       " 'kept',\n",
       " 'voters',\n",
       " 'several',\n",
       " 'associated',\n",
       " 'delegates',\n",
       " 'played',\n",
       " 'puerto',\n",
       " 'rico',\n",
       " 'raising',\n",
       " 'chairman',\n",
       " 'charge',\n",
       " 'facing',\n",
       " 'qaeda',\n",
       " 'pull',\n",
       " 'wounded',\n",
       " 'every',\n",
       " 'information',\n",
       " 'received',\n",
       " 'groups',\n",
       " 'hard',\n",
       " 'rejected',\n",
       " 'average',\n",
       " 'finance',\n",
       " 'africa',\n",
       " 'quest',\n",
       " 'anticipated',\n",
       " 'evidence',\n",
       " 'income',\n",
       " 'hopes',\n",
       " 'name',\n",
       " 'dallas',\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the labels `Y` for train, dev and test sets into arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:03:13.183996Z",
     "start_time": "2020-04-02T15:03:13.077575Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "Y_tr = train_set['label'].tolist()\n",
    "Y_dev = dev_set['label'].tolist()\n",
    "Y_test = test_set['label'].tolist()\n",
    "\n",
    "print(Y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W^T) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i^T $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n",
    "\n",
    "\n",
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers (for the Bonus). Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_classes`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n",
    "\n",
    "See the examples below for expected outputs. Make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:41:20.918617Z",
     "start_time": "2020-04-02T15:41:20.915597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300, \n",
    "                    hidden_dim=[], num_classes=3, init_val = 0.5):\n",
    "    W = {} # Initialise weights dict\n",
    "    W[0] = np.random.uniform(-0.1, 0.1, (vocab_size,embedding_dim)).astype('f') # Initialise embedding layer weights\n",
    "    cur_layer = 1\n",
    "    last_layer_size = embedding_dim\n",
    "    if len(hidden_dim) > 0:\n",
    "        for layer_n in range(len(hidden_dim)):\n",
    "            W[cur_layer] = np.random.uniform(-0.1, 0.1, (last_layer_size,hidden_dim[layer_n])).astype('f') # Initialise hidden layer weights if necessary\n",
    "            last_layer_size = hidden_dim[layer_n]\n",
    "            cur_layer += 1\n",
    "    W[cur_layer] = np.random.uniform(-0.1, 0.1, (last_layer_size,num_classes)).astype('f') # Initialise final layer weights, output is num_classes\n",
    "\n",
    "    return W\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:25:09.113954Z",
     "start_time": "2020-05-11T08:25:09.111854Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_emb: (5, 10)\n",
      "W_out: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=5,embedding_dim=10,hidden_dim=[], num_classes=2)\n",
    "\n",
    "print('W_emb:', W[0].shape)\n",
    "print('W_out:', W[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.636732Z",
     "start_time": "2020-04-02T14:26:48.634122Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[2], num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:25:13.211413Z",
     "start_time": "2020-05-11T08:25:13.209082Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_emb: (3, 4)\n",
      "W_h1: (4, 2)\n",
      "W_out: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print('W_emb:', W[0].shape)\n",
    "print('W_h1:', W[1].shape)\n",
    "print('W_out:', W[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:25:21.153673Z",
     "start_time": "2020-05-11T08:25:21.150953Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08085749,  0.07706536,  0.02544979,  0.04468327],\n",
       "       [-0.09677416,  0.01888638,  0.01135704, -0.06820807],\n",
       "       [-0.06938589,  0.03910591, -0.03624671,  0.03839406]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function (same as in Assignment 1) to be used in the output layer. It takes as input:\n",
    "\n",
    "- `z`: array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:50.504086Z",
     "start_time": "2020-04-02T14:26:50.500686Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    sig = np.exp(z) / np.sum(np.exp(z)) # divide exp of each element by sum of all exp (to get probability)\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss by slightly modifying the function from Assignment 1 to depend only on the true label `y` and the class probabilities vector `y_preds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    l = -1 * np.log(y_preds[y-1]) # log of probability of correct class * -1 (all other classes are not considered)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:25:32.718926Z",
     "start_time": "2020-05-11T08:25:32.716207Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_preds:  [0.01217919 0.27035308 0.24462558 0.02710529 0.44573687]\n",
      "loss: 0.8080264848567503\n"
     ]
    }
   ],
   "source": [
    "# # example for 5 classes\n",
    "\n",
    "y = 5 #true label\n",
    "y_preds = softmax(np.array([[-2.1,1.,0.9,-1.3,1.5]]))[0]\n",
    "\n",
    "print('y_preds: ',y_preds)\n",
    "print('loss:', categorical_loss(y, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network (during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{relu_derivative}(z_i)=\\begin{cases}\n",
    "    0, & \\text{if $z_i<=0$}.\\\\\n",
    "    1, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    z_copy = z.copy()\n",
    "    return np.maximum(z_copy, 0) # if any value is < 0 it will replace it with 0\n",
    "    \n",
    "def relu_derivative(z):\n",
    "    z_copy = z.copy()\n",
    "    \n",
    "    return np.where(z_copy <= 0, 0, 1) # 0 if zi <= 0, otherwise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    dropout_vec = np.ones(size, dtype=int) # Initialise output vector as all ones\n",
    "    zeros_count = int(round(size*dropout_rate,0)) # Obtain number of zeroes\n",
    "    dropout_vec[:zeros_count] = 0 # Make first few elements zeroes\n",
    "    \n",
    "    np.random.shuffle(dropout_vec) # Randomly shuffle the output vector\n",
    "    return dropout_vec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.853632Z",
     "start_time": "2020-04-02T14:26:53.849944Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 0 1 1 1]\n",
      "[1 1 1 1 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_mask(10, 0.2))\n",
    "print(dropout_mask(10, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:54.761268Z",
     "start_time": "2020-04-02T14:26:54.753402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    out_vals = {}\n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_masks = []\n",
    "    input_vec = np.zeros(W[0].shape[0]) # Initialise the one-hot input vector\n",
    "    input_vec[x] = 1 # Make indices that are present 1\n",
    "    h_vecs.append(np.dot(input_vec,W[0])/len(x)) # Vector in first hidden layer\n",
    "    a_vecs.append(relu(h_vecs[0])) # Vector post first hidden layer activation\n",
    "    a_drop_mask = dropout_mask(W[0].shape[1], dropout_rate) # Create dropout mask\n",
    "    dropout_masks.append(a_drop_mask)\n",
    "    dropout_vec = a_vecs[0]*a_drop_mask # Vector post dropout mask\n",
    "    last_hidden_layer_out = dropout_vec\n",
    "    if len(W) > 2: # If there are any hidden layers\n",
    "        for hidden_layer_num in range (1,len(W)-1):\n",
    "            h_vecs.append(np.dot(last_hidden_layer_out,W[hidden_layer_num])) # Vector in hidden layer\n",
    "            a_vecs.append(relu(h_vecs[hidden_layer_num])) # Vector post activation\n",
    "            dropout_masks.append(dropout_mask(W[hidden_layer_num].shape[1], dropout_rate)) # Create dropout mask\n",
    "            last_hidden_layer_out = a_vecs[hidden_layer_num]*dropout_masks[hidden_layer_num]\n",
    "                \n",
    "    output_vec = softmax(np.dot(last_hidden_layer_out,W[len(W)-1])) # Output layer\n",
    "\n",
    "    out_vals['h'] = h_vecs\n",
    "    out_vals['a'] = a_vecs\n",
    "    out_vals['drop'] = dropout_masks\n",
    "    out_vals['y'] = output_vec\n",
    "    \n",
    "    return out_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:24:10.840467Z",
     "start_time": "2020-05-11T08:24:10.837728Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (3, 4)\n",
      "[[ 0.01065147  0.0708905  -0.02303244 -0.03664242]\n",
      " [-0.02914706 -0.06578363  0.06582253 -0.03226583]\n",
      " [ 0.01047402  0.01571029  0.00430661 -0.09946239]]\n",
      "Shape W1 (4, 5)\n",
      "[[ 0.09766909  0.08106831 -0.05847283 -0.04150212  0.00400203]\n",
      " [ 0.08038227  0.09672618 -0.04849159  0.01287181  0.06139374]\n",
      " [-0.02112599  0.04621461 -0.06778619  0.02013971  0.07317289]\n",
      " [ 0.09670432 -0.08412685 -0.01433055 -0.05909143 -0.0098727 ]]\n",
      "Shape W2 (5, 2)\n",
      "[[ 0.00955271 -0.08133466]\n",
      " [-0.04062784  0.08551685]\n",
      " [ 0.01380075 -0.0085176 ]\n",
      " [ 0.0507052   0.04837243]\n",
      " [-0.09028419  0.04173948]]\n",
      "\n",
      "{'h': [array([-0.00933652, -0.02503667,  0.03506457, -0.06586411]), array([-0.00074077,  0.0016205 , -0.00237689,  0.00070619,  0.00256578])], 'a': [array([0.        , 0.        , 0.03506457, 0.        ]), array([0.        , 0.0016205 , 0.        , 0.00070619, 0.00256578])], 'drop': [array([1, 0, 1, 0]), array([0, 0, 1, 1, 1])], 'y': array([0.49991573, 0.50008427])}\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[5], num_classes=2)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "    print((W[i]))\n",
    "print()\n",
    "print(forward_pass([2,1], W, dropout_rate=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and updates the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n",
    "\n",
    "Hint: the gradients on the output layer are similar to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:24:13.732705Z",
     "start_time": "2020-05-11T08:24:13.729741Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (3, 4)\n",
      "[[ 0.09852272 -0.01896296 -0.06196526 -0.0023651 ]\n",
      " [ 0.00047393 -0.01932904 -0.06378092 -0.09039404]\n",
      " [ 0.02093827 -0.04681776  0.0762694   0.01680922]]\n",
      "Shape W1 (4, 5)\n",
      "[[-0.0045079  -0.06569699 -0.088676   -0.06859295 -0.0322765 ]\n",
      " [-0.01187141  0.03676862 -0.05882566  0.02328674  0.08489432]\n",
      " [-0.08407435 -0.00292063  0.03971273 -0.08368631  0.07382047]\n",
      " [-0.01337886 -0.00113399 -0.00085191 -0.02585558  0.08442573]]\n",
      "Shape W2 (5, 2)\n",
      "[[ 0.06825564  0.03110518]\n",
      " [-0.00746299  0.01300226]\n",
      " [-0.0152169  -0.04235761]\n",
      " [-0.04720386  0.03814765]\n",
      " [ 0.06656731  0.01847832]]\n",
      "\n",
      "Weights after backwards:\n",
      "Shape W0 (3, 4)\n",
      "[[ 0.09852272 -0.01896296 -0.06196526 -0.0023651 ]\n",
      " [ 0.00047591 -0.01932904 -0.06378323 -0.09039404]\n",
      " [ 0.02094025 -0.04681776  0.07626709  0.01680922]]\n",
      "Shape W1 (4, 5)\n",
      "[[-0.0045079  -0.06569699 -0.08867614 -0.06859295 -0.03227676]\n",
      " [-0.01187141  0.03676862 -0.05882566  0.02328674  0.08489432]\n",
      " [-0.08407435 -0.00292063  0.03971264 -0.08368631  0.07382032]\n",
      " [-0.01337886 -0.00113399 -0.00085191 -0.02585558  0.08442573]]\n",
      "Shape W2 (5, 2)\n",
      "[[ 0.06825564  0.03110518]\n",
      " [-0.00746299  0.01300226]\n",
      " [-0.01521702 -0.04235748]\n",
      " [-0.04720386  0.03814765]\n",
      " [ 0.06656708  0.01847855]]\n"
     ]
    }
   ],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False):\n",
    "    true_out_vec = np.zeros(out_vals['y'].shape[0]) # Initialise the one-hot prediction vector\n",
    "    true_out_vec[y-1] = 1 # One-hot for ground truth\n",
    "    last_layer_num = len(W)-1\n",
    "    last_error_deriv = (out_vals['y']-true_out_vec) # Get derivative of error function (softmax)\n",
    "    W[last_layer_num] = W[last_layer_num] - lr*np.outer(last_error_deriv, out_vals['a'][last_layer_num-1]).T # Update the final layer's weights\n",
    "    for i in reversed(range(last_layer_num)): # Iterate backwards through weight matrix\n",
    "        last_err_sum = np.sum(last_error_deriv*W[i+1],axis=1) # Sum of previous layer error\n",
    "        last_error_deriv = relu_derivative(out_vals['h'][i]) * last_err_sum # Get derivative of current layer \"error\"\n",
    "        if not freeze_emb and i == 0: # Don't update first layer weights if freeze_emb\n",
    "            input_vec = np.zeros(W[0].shape[0]) # Initialise the one-hot input vector\n",
    "            input_vec[x] = 1 # Make indices that are present 1\n",
    "            W[i] = W[i] - lr*np.outer(last_error_deriv,input_vec).T # Update embedding layer weights    \n",
    "        else:\n",
    "            W[i] = W[i] - lr*np.outer(last_error_deriv,out_vals['a'][i-1]).T # Update hidden layer weights\n",
    "    return W\n",
    "\n",
    "# Testing\n",
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[5], num_classes=2)\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "    print((W[i]))\n",
    "print()\n",
    "out_s = forward_pass([2,1], W, dropout_rate=0.5)\n",
    "W = backward_pass([2,1], 2, W, out_s)\n",
    "print(\"Weights after backwards:\")\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "    print((W[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:19.021428Z",
     "start_time": "2020-04-02T15:09:19.017835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, print_progress=True):\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    prev_val_loss = -1\n",
    "    weights = W\n",
    "    for epoch in range(epochs):  # Train over the training set several times\n",
    "        #Randomly shuffle training and dev sets for this epoch\n",
    "        tr_random = np.random.permutation(len(X_tr))\n",
    "        X_tr = [X_tr[i] for i in tr_random]\n",
    "        Y_tr = [Y_tr[i] for i in tr_random]\n",
    "        if len(X_dev) > 0:\n",
    "            dev_random = np.random.permutation(len(X_dev))\n",
    "            X_dev = [X_dev[i] for i in dev_random]\n",
    "            Y_dev = [Y_dev[i] for i in dev_random]\n",
    "        \n",
    "        if print_progress:\n",
    "            print('Epoch :', epoch+1)\n",
    "        for i, data in enumerate(X_tr): # Training\n",
    "            out_vals = forward_pass(data, weights, dropout_rate=dropout) # Get output vals\n",
    "            weights = backward_pass(data, Y_tr[i], weights, out_vals, lr=lr, freeze_emb=freeze_emb) # Update weights\n",
    "        loss_total = []\n",
    "        for i, data in enumerate(X_tr): # Testing on Train set\n",
    "            out_vals = forward_pass(data, weights, dropout_rate=0)\n",
    "            loss_total.append(categorical_loss(Y_tr[i], out_vals['y']))\n",
    "        training_loss_history.append(np.mean(loss_total))\n",
    "        \n",
    "        loss_total = []\n",
    "        for i, data in enumerate(X_dev): # Testing on Dev set\n",
    "            out_vals = forward_pass(data, weights, dropout_rate=0)\n",
    "            loss_total.append(categorical_loss(Y_dev[i], out_vals['y']))\n",
    "        val_loss = np.mean(loss_total)\n",
    "        if print_progress:\n",
    "            print(f'Dev set validation loss avg: {val_loss}')\n",
    "        validation_loss_history.append(val_loss)\n",
    "        if np.absolute(val_loss - prev_val_loss) <= tolerance:\n",
    "            if print_progress:\n",
    "                print(f'Validation loss stop tolerance reached: {tolerance}')\n",
    "            break\n",
    "        \n",
    "    \n",
    "    \n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate your neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([[-0.00459025, -0.07024013, -0.02946007, ...,  0.04498092,\n",
      "        -0.04821496,  0.08355549],\n",
      "       [-0.06681397, -0.06867211,  0.01513289, ...,  0.08353447,\n",
      "         0.04995019,  0.00116007],\n",
      "       [-0.09952583, -0.0153074 ,  0.0751553 , ...,  0.02776476,\n",
      "         0.04853315, -0.00908589],\n",
      "       ...,\n",
      "       [-0.09399666, -0.02334546,  0.0428985 , ...,  0.06856567,\n",
      "         0.08536387,  0.02200804],\n",
      "       [-0.05042382, -0.00186121, -0.05618834, ...,  0.05496829,\n",
      "        -0.07953812, -0.09062982],\n",
      "       [-0.07887324, -0.02302261, -0.03281147, ..., -0.08712706,\n",
      "         0.0500081 , -0.08320108]], dtype=float32), 1: array([[ 0.01538675, -0.08526607, -0.0001452 ],\n",
      "       [ 0.09793633,  0.00945877,  0.08223426],\n",
      "       [-0.03813721, -0.0494014 ,  0.00290683],\n",
      "       [ 0.00468687,  0.08498226, -0.00105217],\n",
      "       [ 0.04534155,  0.08032256, -0.02064302],\n",
      "       [-0.08544084, -0.07696979, -0.04178055],\n",
      "       [-0.08583806,  0.05613758,  0.07808542],\n",
      "       [ 0.0397662 , -0.07138352, -0.0708855 ],\n",
      "       [-0.08385311, -0.07738077,  0.01488752],\n",
      "       [ 0.00418197,  0.03711877,  0.03899122],\n",
      "       [-0.03573569, -0.08236393, -0.08465848],\n",
      "       [ 0.0520804 ,  0.03641757,  0.00685181],\n",
      "       [-0.03276776,  0.00346899, -0.04643488],\n",
      "       [-0.08645447,  0.08789577, -0.02887632],\n",
      "       [ 0.09503173, -0.00021196,  0.01383226],\n",
      "       [-0.05604652,  0.07700553,  0.02959174],\n",
      "       [ 0.05924659,  0.03197823,  0.03406303],\n",
      "       [ 0.01631329, -0.06273094, -0.07795457],\n",
      "       [ 0.04423262, -0.08346338, -0.09862976],\n",
      "       [ 0.02267777,  0.03304147, -0.02171306],\n",
      "       [-0.03934225, -0.00633884, -0.07032663],\n",
      "       [ 0.05147665, -0.05548306, -0.07567023],\n",
      "       [-0.00231029,  0.01418543, -0.07139294],\n",
      "       [ 0.00171132, -0.02926561, -0.03847068],\n",
      "       [ 0.06450238, -0.00767546,  0.06069059],\n",
      "       [ 0.06961213,  0.08841553, -0.00638778],\n",
      "       [-0.00251714, -0.04321458, -0.00081374],\n",
      "       [ 0.08779152,  0.06827835,  0.05401212],\n",
      "       [ 0.05104554, -0.09108415,  0.07883267],\n",
      "       [-0.08290697,  0.02743868,  0.05368813],\n",
      "       [-0.02483016, -0.06668704, -0.05406293],\n",
      "       [ 0.05213482,  0.02506423,  0.09375729],\n",
      "       [ 0.06540033, -0.05298791, -0.02061627],\n",
      "       [-0.04944801, -0.0586005 , -0.02751727],\n",
      "       [ 0.05171308, -0.07408202, -0.05414752],\n",
      "       [-0.04942095, -0.01383843, -0.04515225],\n",
      "       [-0.08294731, -0.05215796, -0.07091144],\n",
      "       [ 0.04256516,  0.08873009,  0.05890331],\n",
      "       [ 0.06167145,  0.08154822, -0.05140678],\n",
      "       [ 0.0436801 ,  0.09216864, -0.07230951],\n",
      "       [-0.07644724, -0.05660576,  0.01575537],\n",
      "       [ 0.08934284,  0.06295844,  0.01829916],\n",
      "       [ 0.08763346,  0.05917262,  0.02057323],\n",
      "       [ 0.03027727, -0.00741359,  0.07839719],\n",
      "       [ 0.06417781, -0.06554501, -0.06719796],\n",
      "       [-0.08404452,  0.05288522,  0.03491491],\n",
      "       [ 0.04765337,  0.09366332, -0.00597268],\n",
      "       [ 0.06512424,  0.04014485,  0.00376969],\n",
      "       [ 0.06358063,  0.0735821 ,  0.0293173 ],\n",
      "       [-0.01239934, -0.08458633, -0.0788746 ],\n",
      "       [ 0.00380351,  0.08111811, -0.03497608],\n",
      "       [-0.06869564, -0.0047352 ,  0.00424725],\n",
      "       [ 0.0054484 ,  0.00653163, -0.03190942],\n",
      "       [-0.06577362,  0.07844031,  0.06777827],\n",
      "       [-0.00330832,  0.06600349, -0.02425405],\n",
      "       [ 0.03670494, -0.05103612, -0.09584442],\n",
      "       [ 0.06814492, -0.04322219,  0.03014633],\n",
      "       [-0.03176207,  0.06003572,  0.09652188],\n",
      "       [-0.04301807,  0.01119126, -0.09557965],\n",
      "       [ 0.03360152, -0.05412023,  0.01032629],\n",
      "       [-0.02932511, -0.07622719, -0.02968713],\n",
      "       [-0.04559078,  0.06635518,  0.00156149],\n",
      "       [ 0.08308162, -0.02343358,  0.06140196],\n",
      "       [ 0.06288785, -0.09018864,  0.08870189],\n",
      "       [ 0.03159498, -0.03661479,  0.08623427],\n",
      "       [ 0.03434581,  0.00092414,  0.02588676],\n",
      "       [-0.07683229, -0.00148625, -0.01109809],\n",
      "       [-0.03104722,  0.06951407,  0.05076778],\n",
      "       [ 0.03364776,  0.01414617,  0.09303495],\n",
      "       [ 0.09752514,  0.07259883, -0.09709779],\n",
      "       [-0.09402773,  0.07816143,  0.06248229],\n",
      "       [ 0.03674535, -0.01393742,  0.05487087],\n",
      "       [ 0.0351061 , -0.08362901,  0.04395963],\n",
      "       [-0.0954137 , -0.05804534, -0.06028098],\n",
      "       [-0.09468216,  0.01394346,  0.0833162 ],\n",
      "       [-0.09571716, -0.06043022, -0.02919506],\n",
      "       [ 0.03083402, -0.04524011, -0.0633099 ],\n",
      "       [-0.02804676, -0.01791496, -0.03853013],\n",
      "       [ 0.098079  ,  0.01021859, -0.02536676],\n",
      "       [-0.0996718 , -0.06586216, -0.06499657],\n",
      "       [ 0.04661921,  0.02924034,  0.02578178],\n",
      "       [ 0.06077068, -0.02972775, -0.00664253],\n",
      "       [-0.01932193,  0.02121384, -0.07380594],\n",
      "       [-0.02892363,  0.03276681, -0.0921612 ],\n",
      "       [ 0.04487821, -0.00974827,  0.08586879],\n",
      "       [ 0.02981468,  0.00199701,  0.030642  ],\n",
      "       [ 0.07898051,  0.06911945,  0.00588839],\n",
      "       [-0.03171195, -0.0235773 ,  0.01393546],\n",
      "       [-0.01174025,  0.04927513, -0.03954622],\n",
      "       [ 0.01899573,  0.0170138 , -0.01073934],\n",
      "       [-0.02297679,  0.0961877 , -0.01695373],\n",
      "       [ 0.0715626 ,  0.04732168,  0.00832972],\n",
      "       [ 0.02011391, -0.03911701,  0.04890129],\n",
      "       [ 0.00860594,  0.08860497, -0.02066375],\n",
      "       [ 0.0192498 , -0.08097225,  0.08919317],\n",
      "       [ 0.09497887, -0.00062661,  0.02698406],\n",
      "       [ 0.0702289 ,  0.0321977 , -0.03837034],\n",
      "       [-0.07718311, -0.04009086, -0.04114559],\n",
      "       [-0.05774393, -0.03159972, -0.08878963],\n",
      "       [ 0.0817694 ,  0.02851073,  0.01893897],\n",
      "       [ 0.03336321,  0.01070329, -0.05496081],\n",
      "       [ 0.06943627,  0.04709684,  0.07825194],\n",
      "       [-0.03787299,  0.01686272, -0.07194317],\n",
      "       [ 0.00739355,  0.07824308,  0.06216865],\n",
      "       [-0.07380679,  0.00446778, -0.06029021],\n",
      "       [-0.06560771,  0.0363664 , -0.05903959],\n",
      "       [ 0.05846796, -0.04589248, -0.0282777 ],\n",
      "       [ 0.08816493,  0.09464612,  0.00182675],\n",
      "       [ 0.06191145, -0.08183918, -0.0917465 ],\n",
      "       [-0.09968778,  0.06339289, -0.08750536],\n",
      "       [-0.02641923,  0.07797821,  0.05162575],\n",
      "       [-0.00458254, -0.0099109 ,  0.0525887 ],\n",
      "       [ 0.05759393, -0.04011692, -0.07376486],\n",
      "       [ 0.05161648, -0.06512379, -0.02014593],\n",
      "       [-0.06591385, -0.05397028,  0.02085384],\n",
      "       [ 0.06413038,  0.0175505 , -0.08996819],\n",
      "       [-0.09585803,  0.07400228, -0.08980832],\n",
      "       [-0.00267991, -0.0924444 , -0.05186462],\n",
      "       [-0.01515185,  0.03986075, -0.03590131],\n",
      "       [ 0.02995384, -0.06083168,  0.0642137 ],\n",
      "       [ 0.0849863 ,  0.03317135, -0.01505912],\n",
      "       [-0.05945251,  0.03901186, -0.08063836],\n",
      "       [-0.02290052, -0.00592333, -0.01650036],\n",
      "       [-0.06201851,  0.00757006,  0.03870617],\n",
      "       [-0.08974415,  0.0796871 , -0.03835319],\n",
      "       [ 0.01526663, -0.02716155,  0.05059512],\n",
      "       [ 0.0627184 ,  0.01138124,  0.07522938],\n",
      "       [ 0.08436219,  0.05562625, -0.01912616],\n",
      "       [-0.08712175,  0.09979359, -0.00882142],\n",
      "       [-0.08437205, -0.03248692,  0.02424834],\n",
      "       [-0.06588013,  0.0117036 , -0.04534259],\n",
      "       [ 0.04411785,  0.09079938,  0.07858943],\n",
      "       [ 0.07437781, -0.02493928,  0.06769489],\n",
      "       [-0.02032759, -0.02067313,  0.08897746],\n",
      "       [-0.03586926,  0.06125686,  0.04395478],\n",
      "       [-0.08789687,  0.0129961 , -0.08389614],\n",
      "       [-0.09066029,  0.0898554 ,  0.02829932],\n",
      "       [ 0.08163767,  0.03897053, -0.05297661],\n",
      "       [-0.06057362, -0.0076702 ,  0.09562765],\n",
      "       [-0.07141784,  0.0259405 ,  0.02688174],\n",
      "       [-0.03055306, -0.04824502,  0.01220145],\n",
      "       [-0.08441211,  0.01257654,  0.00909374],\n",
      "       [ 0.05507461, -0.04031416,  0.08706016],\n",
      "       [ 0.01124081,  0.02576645,  0.06172876],\n",
      "       [-0.09830678, -0.04265005,  0.06681825],\n",
      "       [ 0.08533546,  0.09937018, -0.01558363],\n",
      "       [-0.0762322 , -0.07026158, -0.08087619],\n",
      "       [ 0.07737032, -0.02934619,  0.04952491],\n",
      "       [ 0.01083442,  0.02250759,  0.00576493],\n",
      "       [-0.04761399,  0.01008658, -0.04397853],\n",
      "       [-0.00295298, -0.04911513, -0.06419858],\n",
      "       [ 0.09370726,  0.0567109 , -0.02899212],\n",
      "       [-0.02443735, -0.03355455,  0.0839342 ],\n",
      "       [-0.09883268,  0.07148135,  0.06258386],\n",
      "       [ 0.03024705, -0.02460416,  0.07132223],\n",
      "       [-0.00935624, -0.09860929, -0.01418565],\n",
      "       [-0.07599691,  0.03766003, -0.08213784],\n",
      "       [ 0.09955825,  0.07886057,  0.07619245],\n",
      "       [-0.05951497,  0.09247175,  0.02273495],\n",
      "       [ 0.06296561,  0.04073468,  0.05277248],\n",
      "       [ 0.05692435,  0.04206699, -0.08061705],\n",
      "       [ 0.09990532, -0.05490556,  0.02439899],\n",
      "       [ 0.06417198, -0.05784682, -0.00520399],\n",
      "       [-0.03898094, -0.00050734, -0.03499546],\n",
      "       [-0.08501098,  0.09050113,  0.05386458],\n",
      "       [-0.09011921, -0.00845925,  0.07118398],\n",
      "       [-0.04191255, -0.01147493,  0.05484868],\n",
      "       [-0.00922287,  0.01000758, -0.04867342],\n",
      "       [ 0.02696033, -0.0191965 , -0.00443156],\n",
      "       [ 0.01858143, -0.01883208,  0.09181488],\n",
      "       [-0.08634193,  0.00684891, -0.06868416],\n",
      "       [-0.06016193,  0.0591706 , -0.05354718],\n",
      "       [-0.08353073, -0.05319206,  0.03097171],\n",
      "       [-0.08641342, -0.01350778,  0.09609205],\n",
      "       [-0.04165843, -0.09386208, -0.00993782],\n",
      "       [ 0.07102651, -0.0407442 , -0.08490337],\n",
      "       [ 0.05358264,  0.06174397,  0.06275947],\n",
      "       [ 0.05760389,  0.01833591,  0.02125223],\n",
      "       [-0.05907536,  0.00065728, -0.04639161],\n",
      "       [ 0.03731765, -0.07688131, -0.05833094],\n",
      "       [-0.01616097,  0.07941037,  0.01396044],\n",
      "       [-0.06558936,  0.06167009,  0.05504043],\n",
      "       [-0.04473786,  0.02101139, -0.08163557],\n",
      "       [ 0.03082038, -0.01632136,  0.09196291],\n",
      "       [ 0.04638844,  0.04389106, -0.05088715],\n",
      "       [ 0.0394683 ,  0.09283262, -0.02988687],\n",
      "       [ 0.02216333, -0.08871677, -0.04713756],\n",
      "       [-0.0394677 ,  0.01056379, -0.03906949],\n",
      "       [-0.02521628, -0.02745073, -0.04188187],\n",
      "       [ 0.08401454, -0.02806075,  0.03626778],\n",
      "       [ 0.01390513,  0.07683195,  0.03543993],\n",
      "       [ 0.09551722,  0.0752036 , -0.07219613],\n",
      "       [ 0.02472668,  0.03044768, -0.01932996],\n",
      "       [ 0.05627073,  0.00617927, -0.0726204 ],\n",
      "       [ 0.03107594,  0.06671723,  0.05041227],\n",
      "       [-0.02250713, -0.04608863,  0.02466863],\n",
      "       [-0.00288569, -0.03511437,  0.09503098],\n",
      "       [-0.08496352,  0.07520566, -0.00565571],\n",
      "       [-0.03637248, -0.09184143,  0.08066028],\n",
      "       [-0.0486227 ,  0.02712038,  0.03199804],\n",
      "       [-0.00307501,  0.02790595,  0.09030712],\n",
      "       [-0.04428986,  0.02317333,  0.07900086],\n",
      "       [-0.06678571,  0.08384563,  0.0273051 ],\n",
      "       [ 0.06110137, -0.03595086, -0.00777069],\n",
      "       [-0.01711076,  0.07090889, -0.09773272],\n",
      "       [-0.04039858,  0.0437201 , -0.01288852],\n",
      "       [ 0.0699343 ,  0.02468455, -0.09647323],\n",
      "       [ 0.09351368,  0.00979544,  0.00570286],\n",
      "       [-0.0407885 , -0.09343597,  0.00490499],\n",
      "       [ 0.04909586, -0.01184317,  0.01619871],\n",
      "       [ 0.09554443,  0.00116091,  0.04628741],\n",
      "       [ 0.02483897, -0.00420922,  0.06125483],\n",
      "       [ 0.05231859, -0.04280364, -0.08622176],\n",
      "       [ 0.02856031,  0.04410973, -0.00359401],\n",
      "       [-0.00492739, -0.060798  ,  0.09026595],\n",
      "       [ 0.03133063,  0.02228422,  0.06862394],\n",
      "       [ 0.03577587,  0.0985342 , -0.09855253],\n",
      "       [-0.01682893,  0.03080339, -0.02211257],\n",
      "       [ 0.09424065,  0.01727177,  0.03497781],\n",
      "       [-0.04181954,  0.0283768 ,  0.01052806],\n",
      "       [-0.04554101, -0.09518615, -0.08376094],\n",
      "       [ 0.09495069, -0.08744608,  0.00828852],\n",
      "       [ 0.09356889, -0.00107709,  0.06412391],\n",
      "       [ 0.04881715,  0.07428022, -0.04499986],\n",
      "       [ 0.0104336 ,  0.05555189,  0.0711626 ],\n",
      "       [-0.0056813 ,  0.09101052, -0.03755713],\n",
      "       [-0.01508474, -0.04429275, -0.08333036],\n",
      "       [ 0.0296794 , -0.04518873, -0.09990738],\n",
      "       [-0.07937582, -0.02115364, -0.08421024],\n",
      "       [-0.07396249, -0.05573713,  0.03473246],\n",
      "       [ 0.06122297, -0.00081446,  0.06750398],\n",
      "       [ 0.00294045, -0.02942935,  0.08851824],\n",
      "       [ 0.07320569,  0.01639596,  0.03921208],\n",
      "       [-0.06660176, -0.08645508, -0.0424257 ],\n",
      "       [-0.01184353,  0.09340837, -0.0013083 ],\n",
      "       [-0.09850895,  0.05150999,  0.02658016],\n",
      "       [ 0.08122087, -0.00420189,  0.02268195],\n",
      "       [-0.0136282 , -0.01229324,  0.00970869],\n",
      "       [ 0.05287201, -0.07545566,  0.01728655],\n",
      "       [ 0.03139161, -0.02017396, -0.08218917],\n",
      "       [-0.01911475, -0.07678536,  0.09496304],\n",
      "       [ 0.08178274,  0.09910157,  0.01475442],\n",
      "       [ 0.08795069, -0.01063152, -0.09590109],\n",
      "       [-0.09141181, -0.07435655, -0.00925332],\n",
      "       [ 0.07643018,  0.07011749, -0.05251823],\n",
      "       [-0.06886495, -0.00616271,  0.06578643],\n",
      "       [ 0.06130085, -0.04378425,  0.09634586],\n",
      "       [-0.00801215,  0.00716849,  0.04042326],\n",
      "       [-0.0404463 , -0.04810171,  0.08991028],\n",
      "       [ 0.00257248,  0.05166089,  0.0100502 ],\n",
      "       [ 0.018039  , -0.05855477, -0.04833286],\n",
      "       [ 0.00340391,  0.02466111,  0.07983723],\n",
      "       [-0.06071269,  0.06970559,  0.07715427],\n",
      "       [ 0.01805665, -0.06794696,  0.09809213],\n",
      "       [ 0.07814649, -0.08688172, -0.08618012],\n",
      "       [ 0.07296707, -0.07869159, -0.08491962],\n",
      "       [-0.07184905,  0.00531198, -0.07350449],\n",
      "       [-0.05138914,  0.01905974,  0.02690551],\n",
      "       [ 0.09210174, -0.07679678, -0.0220272 ],\n",
      "       [ 0.03596015, -0.09185288, -0.0415706 ],\n",
      "       [-0.09775694, -0.09891614, -0.0012601 ],\n",
      "       [-0.04063318, -0.07147364, -0.02260291],\n",
      "       [-0.07262572,  0.06016704,  0.07006085],\n",
      "       [ 0.03927004, -0.00043316, -0.00590188],\n",
      "       [-0.03887847, -0.06564965, -0.00024767],\n",
      "       [ 0.01023269, -0.08642415, -0.09814413],\n",
      "       [ 0.09642664, -0.09510937, -0.03716251],\n",
      "       [ 0.00386825, -0.03005777,  0.00555453],\n",
      "       [-0.07977264, -0.06708999,  0.03537562],\n",
      "       [ 0.09822244,  0.02098433, -0.06461786],\n",
      "       [ 0.01479851, -0.01743016,  0.00446374],\n",
      "       [ 0.05371904,  0.02890253,  0.06616146],\n",
      "       [ 0.06118734, -0.00283904,  0.07381372],\n",
      "       [ 0.08397242, -0.00652124, -0.06295293],\n",
      "       [-0.0241392 ,  0.06254766,  0.04881721],\n",
      "       [-0.00554219,  0.00864562,  0.07405284],\n",
      "       [ 0.05094349,  0.058261  , -0.06181683],\n",
      "       [ 0.01156594,  0.05516258,  0.02837261],\n",
      "       [ 0.08418472,  0.09564134, -0.09522732],\n",
      "       [ 0.03357087, -0.02458881,  0.02847256],\n",
      "       [ 0.03759738, -0.07790592,  0.04877819],\n",
      "       [ 0.0421899 ,  0.02736316, -0.0403974 ],\n",
      "       [ 0.05501125,  0.00379572,  0.08196051],\n",
      "       [ 0.08996001,  0.08106443,  0.03276949],\n",
      "       [-0.06085404, -0.04264566, -0.05383168],\n",
      "       [ 0.0691284 ,  0.0412777 , -0.09734838],\n",
      "       [-0.07152459, -0.08630776, -0.03940894],\n",
      "       [-0.0032045 ,  0.06576041, -0.02391306],\n",
      "       [-0.03442147,  0.01610834, -0.01923272],\n",
      "       [-0.00874866,  0.08494107,  0.08392322],\n",
      "       [ 0.01950028,  0.0814568 ,  0.01426535],\n",
      "       [ 0.05383019,  0.03646505,  0.00670211],\n",
      "       [ 0.09166887, -0.06487349,  0.05263192],\n",
      "       [-0.08367904, -0.07765318,  0.03780546],\n",
      "       [ 0.09584218,  0.06472444, -0.06467539],\n",
      "       [ 0.04752963,  0.08041404, -0.06817415],\n",
      "       [-0.05191286, -0.04016518, -0.08391884],\n",
      "       [ 0.03972578,  0.06904867,  0.03882167],\n",
      "       [ 0.04018662,  0.09578238,  0.02961439],\n",
      "       [ 0.07142606,  0.02521397, -0.0965448 ]], dtype=float32)}\n",
      "{0: array([[-0.00459025, -0.07024013, -0.02946007, ...,  0.04498092,\n",
      "        -0.04821496,  0.08355549],\n",
      "       [-0.06681397, -0.06867211,  0.01513289, ...,  0.08353447,\n",
      "         0.04995019,  0.00116007],\n",
      "       [-0.09952583, -0.0153074 ,  0.0751553 , ...,  0.02776476,\n",
      "         0.04853315, -0.00908589],\n",
      "       ...,\n",
      "       [-0.09399666, -0.02334546,  0.0428985 , ...,  0.06856567,\n",
      "         0.08536387,  0.02200804],\n",
      "       [-0.05042382, -0.00186121, -0.05618834, ...,  0.05496829,\n",
      "        -0.07953812, -0.09062982],\n",
      "       [-0.07887324, -0.02302261, -0.03281147, ..., -0.08712706,\n",
      "         0.0500081 , -0.08320108]], dtype=float32), 1: array([[ 0.01538675, -0.08526607, -0.0001452 ],\n",
      "       [ 0.09793633,  0.00945877,  0.08223426],\n",
      "       [-0.03813721, -0.0494014 ,  0.00290683],\n",
      "       [ 0.00468687,  0.08498226, -0.00105217],\n",
      "       [ 0.04534155,  0.08032256, -0.02064302],\n",
      "       [-0.08544084, -0.07696979, -0.04178055],\n",
      "       [-0.08583806,  0.05613758,  0.07808542],\n",
      "       [ 0.0397662 , -0.07138352, -0.0708855 ],\n",
      "       [-0.08385311, -0.07738077,  0.01488752],\n",
      "       [ 0.00418197,  0.03711877,  0.03899122],\n",
      "       [-0.03573569, -0.08236393, -0.08465848],\n",
      "       [ 0.0520804 ,  0.03641757,  0.00685181],\n",
      "       [-0.03276776,  0.00346899, -0.04643488],\n",
      "       [-0.08645447,  0.08789577, -0.02887632],\n",
      "       [ 0.09503173, -0.00021196,  0.01383226],\n",
      "       [-0.05604652,  0.07700553,  0.02959174],\n",
      "       [ 0.05924659,  0.03197823,  0.03406303],\n",
      "       [ 0.01631329, -0.06273094, -0.07795457],\n",
      "       [ 0.04423262, -0.08346338, -0.09862976],\n",
      "       [ 0.02267777,  0.03304147, -0.02171306],\n",
      "       [-0.03934225, -0.00633884, -0.07032663],\n",
      "       [ 0.05147665, -0.05548306, -0.07567023],\n",
      "       [-0.00231029,  0.01418543, -0.07139294],\n",
      "       [ 0.00171132, -0.02926561, -0.03847068],\n",
      "       [ 0.06450238, -0.00767546,  0.06069059],\n",
      "       [ 0.06961213,  0.08841553, -0.00638778],\n",
      "       [-0.00251714, -0.04321458, -0.00081374],\n",
      "       [ 0.08779152,  0.06827835,  0.05401212],\n",
      "       [ 0.05104554, -0.09108415,  0.07883267],\n",
      "       [-0.08290697,  0.02743868,  0.05368813],\n",
      "       [-0.02483016, -0.06668704, -0.05406293],\n",
      "       [ 0.05213482,  0.02506423,  0.09375729],\n",
      "       [ 0.06540033, -0.05298791, -0.02061627],\n",
      "       [-0.04944801, -0.0586005 , -0.02751727],\n",
      "       [ 0.05171308, -0.07408202, -0.05414752],\n",
      "       [-0.04942095, -0.01383843, -0.04515225],\n",
      "       [-0.08294731, -0.05215796, -0.07091144],\n",
      "       [ 0.04256516,  0.08873009,  0.05890331],\n",
      "       [ 0.06167145,  0.08154822, -0.05140678],\n",
      "       [ 0.0436801 ,  0.09216864, -0.07230951],\n",
      "       [-0.07644724, -0.05660576,  0.01575537],\n",
      "       [ 0.08934284,  0.06295844,  0.01829916],\n",
      "       [ 0.08763346,  0.05917262,  0.02057323],\n",
      "       [ 0.03027727, -0.00741359,  0.07839719],\n",
      "       [ 0.06417781, -0.06554501, -0.06719796],\n",
      "       [-0.08404452,  0.05288522,  0.03491491],\n",
      "       [ 0.04765337,  0.09366332, -0.00597268],\n",
      "       [ 0.06512424,  0.04014485,  0.00376969],\n",
      "       [ 0.06358063,  0.0735821 ,  0.0293173 ],\n",
      "       [-0.01239934, -0.08458633, -0.0788746 ],\n",
      "       [ 0.00380351,  0.08111811, -0.03497608],\n",
      "       [-0.06869564, -0.0047352 ,  0.00424725],\n",
      "       [ 0.0054484 ,  0.00653163, -0.03190942],\n",
      "       [-0.06577362,  0.07844031,  0.06777827],\n",
      "       [-0.00330832,  0.06600349, -0.02425405],\n",
      "       [ 0.03670494, -0.05103612, -0.09584442],\n",
      "       [ 0.06814492, -0.04322219,  0.03014633],\n",
      "       [-0.03176207,  0.06003572,  0.09652188],\n",
      "       [-0.04301807,  0.01119126, -0.09557965],\n",
      "       [ 0.03360152, -0.05412023,  0.01032629],\n",
      "       [-0.02932511, -0.07622719, -0.02968713],\n",
      "       [-0.04559078,  0.06635518,  0.00156149],\n",
      "       [ 0.08308162, -0.02343358,  0.06140196],\n",
      "       [ 0.06288785, -0.09018864,  0.08870189],\n",
      "       [ 0.03159498, -0.03661479,  0.08623427],\n",
      "       [ 0.03434581,  0.00092414,  0.02588676],\n",
      "       [-0.07683229, -0.00148625, -0.01109809],\n",
      "       [-0.03104722,  0.06951407,  0.05076778],\n",
      "       [ 0.03364776,  0.01414617,  0.09303495],\n",
      "       [ 0.09752514,  0.07259883, -0.09709779],\n",
      "       [-0.09402773,  0.07816143,  0.06248229],\n",
      "       [ 0.03674535, -0.01393742,  0.05487087],\n",
      "       [ 0.0351061 , -0.08362901,  0.04395963],\n",
      "       [-0.0954137 , -0.05804534, -0.06028098],\n",
      "       [-0.09468216,  0.01394346,  0.0833162 ],\n",
      "       [-0.09571716, -0.06043022, -0.02919506],\n",
      "       [ 0.03083402, -0.04524011, -0.0633099 ],\n",
      "       [-0.02804676, -0.01791496, -0.03853013],\n",
      "       [ 0.098079  ,  0.01021859, -0.02536676],\n",
      "       [-0.0996718 , -0.06586216, -0.06499657],\n",
      "       [ 0.04661921,  0.02924034,  0.02578178],\n",
      "       [ 0.06077068, -0.02972775, -0.00664253],\n",
      "       [-0.01932193,  0.02121384, -0.07380594],\n",
      "       [-0.02892363,  0.03276681, -0.0921612 ],\n",
      "       [ 0.04487821, -0.00974827,  0.08586879],\n",
      "       [ 0.02981468,  0.00199701,  0.030642  ],\n",
      "       [ 0.07898051,  0.06911945,  0.00588839],\n",
      "       [-0.03171195, -0.0235773 ,  0.01393546],\n",
      "       [-0.01174025,  0.04927513, -0.03954622],\n",
      "       [ 0.01899573,  0.0170138 , -0.01073934],\n",
      "       [-0.02297679,  0.0961877 , -0.01695373],\n",
      "       [ 0.0715626 ,  0.04732168,  0.00832972],\n",
      "       [ 0.02011391, -0.03911701,  0.04890129],\n",
      "       [ 0.00860594,  0.08860497, -0.02066375],\n",
      "       [ 0.0192498 , -0.08097225,  0.08919317],\n",
      "       [ 0.09497887, -0.00062661,  0.02698406],\n",
      "       [ 0.0702289 ,  0.0321977 , -0.03837034],\n",
      "       [-0.07718311, -0.04009086, -0.04114559],\n",
      "       [-0.05774393, -0.03159972, -0.08878963],\n",
      "       [ 0.0817694 ,  0.02851073,  0.01893897],\n",
      "       [ 0.03336321,  0.01070329, -0.05496081],\n",
      "       [ 0.06943627,  0.04709684,  0.07825194],\n",
      "       [-0.03787299,  0.01686272, -0.07194317],\n",
      "       [ 0.00739355,  0.07824308,  0.06216865],\n",
      "       [-0.07380679,  0.00446778, -0.06029021],\n",
      "       [-0.06560771,  0.0363664 , -0.05903959],\n",
      "       [ 0.05846796, -0.04589248, -0.0282777 ],\n",
      "       [ 0.08816493,  0.09464612,  0.00182675],\n",
      "       [ 0.06191145, -0.08183918, -0.0917465 ],\n",
      "       [-0.09968778,  0.06339289, -0.08750536],\n",
      "       [-0.02641923,  0.07797821,  0.05162575],\n",
      "       [-0.00458254, -0.0099109 ,  0.0525887 ],\n",
      "       [ 0.05759393, -0.04011692, -0.07376486],\n",
      "       [ 0.05161648, -0.06512379, -0.02014593],\n",
      "       [-0.06591385, -0.05397028,  0.02085384],\n",
      "       [ 0.06413038,  0.0175505 , -0.08996819],\n",
      "       [-0.09585803,  0.07400228, -0.08980832],\n",
      "       [-0.00267991, -0.0924444 , -0.05186462],\n",
      "       [-0.01515185,  0.03986075, -0.03590131],\n",
      "       [ 0.02995384, -0.06083168,  0.0642137 ],\n",
      "       [ 0.0849863 ,  0.03317135, -0.01505912],\n",
      "       [-0.05945251,  0.03901186, -0.08063836],\n",
      "       [-0.02290052, -0.00592333, -0.01650036],\n",
      "       [-0.06201851,  0.00757006,  0.03870617],\n",
      "       [-0.08974415,  0.0796871 , -0.03835319],\n",
      "       [ 0.01526663, -0.02716155,  0.05059512],\n",
      "       [ 0.0627184 ,  0.01138124,  0.07522938],\n",
      "       [ 0.08436219,  0.05562625, -0.01912616],\n",
      "       [-0.08712175,  0.09979359, -0.00882142],\n",
      "       [-0.08437205, -0.03248692,  0.02424834],\n",
      "       [-0.06588013,  0.0117036 , -0.04534259],\n",
      "       [ 0.04411785,  0.09079938,  0.07858943],\n",
      "       [ 0.07437781, -0.02493928,  0.06769489],\n",
      "       [-0.02032759, -0.02067313,  0.08897746],\n",
      "       [-0.03586926,  0.06125686,  0.04395478],\n",
      "       [-0.08789687,  0.0129961 , -0.08389614],\n",
      "       [-0.09066029,  0.0898554 ,  0.02829932],\n",
      "       [ 0.08163767,  0.03897053, -0.05297661],\n",
      "       [-0.06057362, -0.0076702 ,  0.09562765],\n",
      "       [-0.07141784,  0.0259405 ,  0.02688174],\n",
      "       [-0.03055306, -0.04824502,  0.01220145],\n",
      "       [-0.08441211,  0.01257654,  0.00909374],\n",
      "       [ 0.05507461, -0.04031416,  0.08706016],\n",
      "       [ 0.01124081,  0.02576645,  0.06172876],\n",
      "       [-0.09830678, -0.04265005,  0.06681825],\n",
      "       [ 0.08533546,  0.09937018, -0.01558363],\n",
      "       [-0.0762322 , -0.07026158, -0.08087619],\n",
      "       [ 0.07737032, -0.02934619,  0.04952491],\n",
      "       [ 0.01083442,  0.02250759,  0.00576493],\n",
      "       [-0.04761399,  0.01008658, -0.04397853],\n",
      "       [-0.00295298, -0.04911513, -0.06419858],\n",
      "       [ 0.09370726,  0.0567109 , -0.02899212],\n",
      "       [-0.02443735, -0.03355455,  0.0839342 ],\n",
      "       [-0.09883268,  0.07148135,  0.06258386],\n",
      "       [ 0.03024705, -0.02460416,  0.07132223],\n",
      "       [-0.00935624, -0.09860929, -0.01418565],\n",
      "       [-0.07599691,  0.03766003, -0.08213784],\n",
      "       [ 0.09955825,  0.07886057,  0.07619245],\n",
      "       [-0.05951497,  0.09247175,  0.02273495],\n",
      "       [ 0.06296561,  0.04073468,  0.05277248],\n",
      "       [ 0.05692435,  0.04206699, -0.08061705],\n",
      "       [ 0.09990532, -0.05490556,  0.02439899],\n",
      "       [ 0.06417198, -0.05784682, -0.00520399],\n",
      "       [-0.03898094, -0.00050734, -0.03499546],\n",
      "       [-0.08501098,  0.09050113,  0.05386458],\n",
      "       [-0.09011921, -0.00845925,  0.07118398],\n",
      "       [-0.04191255, -0.01147493,  0.05484868],\n",
      "       [-0.00922287,  0.01000758, -0.04867342],\n",
      "       [ 0.02696033, -0.0191965 , -0.00443156],\n",
      "       [ 0.01858143, -0.01883208,  0.09181488],\n",
      "       [-0.08634193,  0.00684891, -0.06868416],\n",
      "       [-0.06016193,  0.0591706 , -0.05354718],\n",
      "       [-0.08353073, -0.05319206,  0.03097171],\n",
      "       [-0.08641342, -0.01350778,  0.09609205],\n",
      "       [-0.04165843, -0.09386208, -0.00993782],\n",
      "       [ 0.07102651, -0.0407442 , -0.08490337],\n",
      "       [ 0.05358264,  0.06174397,  0.06275947],\n",
      "       [ 0.05760389,  0.01833591,  0.02125223],\n",
      "       [-0.05907536,  0.00065728, -0.04639161],\n",
      "       [ 0.03731765, -0.07688131, -0.05833094],\n",
      "       [-0.01616097,  0.07941037,  0.01396044],\n",
      "       [-0.06558936,  0.06167009,  0.05504043],\n",
      "       [-0.04473786,  0.02101139, -0.08163557],\n",
      "       [ 0.03082038, -0.01632136,  0.09196291],\n",
      "       [ 0.04638844,  0.04389106, -0.05088715],\n",
      "       [ 0.0394683 ,  0.09283262, -0.02988687],\n",
      "       [ 0.02216333, -0.08871677, -0.04713756],\n",
      "       [-0.0394677 ,  0.01056379, -0.03906949],\n",
      "       [-0.02521628, -0.02745073, -0.04188187],\n",
      "       [ 0.08401454, -0.02806075,  0.03626778],\n",
      "       [ 0.01390513,  0.07683195,  0.03543993],\n",
      "       [ 0.09551722,  0.0752036 , -0.07219613],\n",
      "       [ 0.02472668,  0.03044768, -0.01932996],\n",
      "       [ 0.05627073,  0.00617927, -0.0726204 ],\n",
      "       [ 0.03107594,  0.06671723,  0.05041227],\n",
      "       [-0.02250713, -0.04608863,  0.02466863],\n",
      "       [-0.00288569, -0.03511437,  0.09503098],\n",
      "       [-0.08496352,  0.07520566, -0.00565571],\n",
      "       [-0.03637248, -0.09184143,  0.08066028],\n",
      "       [-0.0486227 ,  0.02712038,  0.03199804],\n",
      "       [-0.00307501,  0.02790595,  0.09030712],\n",
      "       [-0.04428986,  0.02317333,  0.07900086],\n",
      "       [-0.06678571,  0.08384563,  0.0273051 ],\n",
      "       [ 0.06110137, -0.03595086, -0.00777069],\n",
      "       [-0.01711076,  0.07090889, -0.09773272],\n",
      "       [-0.04039858,  0.0437201 , -0.01288852],\n",
      "       [ 0.0699343 ,  0.02468455, -0.09647323],\n",
      "       [ 0.09351368,  0.00979544,  0.00570286],\n",
      "       [-0.0407885 , -0.09343597,  0.00490499],\n",
      "       [ 0.04909586, -0.01184317,  0.01619871],\n",
      "       [ 0.09554443,  0.00116091,  0.04628741],\n",
      "       [ 0.02483897, -0.00420922,  0.06125483],\n",
      "       [ 0.05231859, -0.04280364, -0.08622176],\n",
      "       [ 0.02856031,  0.04410973, -0.00359401],\n",
      "       [-0.00492739, -0.060798  ,  0.09026595],\n",
      "       [ 0.03133063,  0.02228422,  0.06862394],\n",
      "       [ 0.03577587,  0.0985342 , -0.09855253],\n",
      "       [-0.01682893,  0.03080339, -0.02211257],\n",
      "       [ 0.09424065,  0.01727177,  0.03497781],\n",
      "       [-0.04181954,  0.0283768 ,  0.01052806],\n",
      "       [-0.04554101, -0.09518615, -0.08376094],\n",
      "       [ 0.09495069, -0.08744608,  0.00828852],\n",
      "       [ 0.09356889, -0.00107709,  0.06412391],\n",
      "       [ 0.04881715,  0.07428022, -0.04499986],\n",
      "       [ 0.0104336 ,  0.05555189,  0.0711626 ],\n",
      "       [-0.0056813 ,  0.09101052, -0.03755713],\n",
      "       [-0.01508474, -0.04429275, -0.08333036],\n",
      "       [ 0.0296794 , -0.04518873, -0.09990738],\n",
      "       [-0.07937582, -0.02115364, -0.08421024],\n",
      "       [-0.07396249, -0.05573713,  0.03473246],\n",
      "       [ 0.06122297, -0.00081446,  0.06750398],\n",
      "       [ 0.00294045, -0.02942935,  0.08851824],\n",
      "       [ 0.07320569,  0.01639596,  0.03921208],\n",
      "       [-0.06660176, -0.08645508, -0.0424257 ],\n",
      "       [-0.01184353,  0.09340837, -0.0013083 ],\n",
      "       [-0.09850895,  0.05150999,  0.02658016],\n",
      "       [ 0.08122087, -0.00420189,  0.02268195],\n",
      "       [-0.0136282 , -0.01229324,  0.00970869],\n",
      "       [ 0.05287201, -0.07545566,  0.01728655],\n",
      "       [ 0.03139161, -0.02017396, -0.08218917],\n",
      "       [-0.01911475, -0.07678536,  0.09496304],\n",
      "       [ 0.08178274,  0.09910157,  0.01475442],\n",
      "       [ 0.08795069, -0.01063152, -0.09590109],\n",
      "       [-0.09141181, -0.07435655, -0.00925332],\n",
      "       [ 0.07643018,  0.07011749, -0.05251823],\n",
      "       [-0.06886495, -0.00616271,  0.06578643],\n",
      "       [ 0.06130085, -0.04378425,  0.09634586],\n",
      "       [-0.00801215,  0.00716849,  0.04042326],\n",
      "       [-0.0404463 , -0.04810171,  0.08991028],\n",
      "       [ 0.00257248,  0.05166089,  0.0100502 ],\n",
      "       [ 0.018039  , -0.05855477, -0.04833286],\n",
      "       [ 0.00340391,  0.02466111,  0.07983723],\n",
      "       [-0.06071269,  0.06970559,  0.07715427],\n",
      "       [ 0.01805665, -0.06794696,  0.09809213],\n",
      "       [ 0.07814649, -0.08688172, -0.08618012],\n",
      "       [ 0.07296707, -0.07869159, -0.08491962],\n",
      "       [-0.07184905,  0.00531198, -0.07350449],\n",
      "       [-0.05138914,  0.01905974,  0.02690551],\n",
      "       [ 0.09210174, -0.07679678, -0.0220272 ],\n",
      "       [ 0.03596015, -0.09185288, -0.0415706 ],\n",
      "       [-0.09775694, -0.09891614, -0.0012601 ],\n",
      "       [-0.04063318, -0.07147364, -0.02260291],\n",
      "       [-0.07262572,  0.06016704,  0.07006085],\n",
      "       [ 0.03927004, -0.00043316, -0.00590188],\n",
      "       [-0.03887847, -0.06564965, -0.00024767],\n",
      "       [ 0.01023269, -0.08642415, -0.09814413],\n",
      "       [ 0.09642664, -0.09510937, -0.03716251],\n",
      "       [ 0.00386825, -0.03005777,  0.00555453],\n",
      "       [-0.07977264, -0.06708999,  0.03537562],\n",
      "       [ 0.09822244,  0.02098433, -0.06461786],\n",
      "       [ 0.01479851, -0.01743016,  0.00446374],\n",
      "       [ 0.05371904,  0.02890253,  0.06616146],\n",
      "       [ 0.06118734, -0.00283904,  0.07381372],\n",
      "       [ 0.08397242, -0.00652124, -0.06295293],\n",
      "       [-0.0241392 ,  0.06254766,  0.04881721],\n",
      "       [-0.00554219,  0.00864562,  0.07405284],\n",
      "       [ 0.05094349,  0.058261  , -0.06181683],\n",
      "       [ 0.01156594,  0.05516258,  0.02837261],\n",
      "       [ 0.08418472,  0.09564134, -0.09522732],\n",
      "       [ 0.03357087, -0.02458881,  0.02847256],\n",
      "       [ 0.03759738, -0.07790592,  0.04877819],\n",
      "       [ 0.0421899 ,  0.02736316, -0.0403974 ],\n",
      "       [ 0.05501125,  0.00379572,  0.08196051],\n",
      "       [ 0.08996001,  0.08106443,  0.03276949],\n",
      "       [-0.06085404, -0.04264566, -0.05383168],\n",
      "       [ 0.0691284 ,  0.0412777 , -0.09734838],\n",
      "       [-0.07152459, -0.08630776, -0.03940894],\n",
      "       [-0.0032045 ,  0.06576041, -0.02391306],\n",
      "       [-0.03442147,  0.01610834, -0.01923272],\n",
      "       [-0.00874866,  0.08494107,  0.08392322],\n",
      "       [ 0.01950028,  0.0814568 ,  0.01426535],\n",
      "       [ 0.05383019,  0.03646505,  0.00670211],\n",
      "       [ 0.09166887, -0.06487349,  0.05263192],\n",
      "       [-0.08367904, -0.07765318,  0.03780546],\n",
      "       [ 0.09584218,  0.06472444, -0.06467539],\n",
      "       [ 0.04752963,  0.08041404, -0.06817415],\n",
      "       [-0.05191286, -0.04016518, -0.08391884],\n",
      "       [ 0.03972578,  0.06904867,  0.03882167],\n",
      "       [ 0.04018662,  0.09578238,  0.02961439],\n",
      "       [ 0.07142606,  0.02521397, -0.0965448 ]], dtype=float32)}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (6238, 300)\n",
      "Shape W1 (300, 3)\n",
      "Trying params: LR 0.001, drop rate 0.1\n",
      "Epoch : 1\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'weights' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jacob\\OneDrive\\Documents\\SHEFFIELD_2022\\NLP\\assignment.ipynb Cell 51'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, drop \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dropout_rate_values):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrying params: LR \u001b[39m\u001b[39m{\u001b[39;00meta\u001b[39m}\u001b[39;00m\u001b[39m, drop rate \u001b[39m\u001b[39m{\u001b[39;00mdrop\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=15'>16</a>\u001b[0m     W_out, loss_tr_temp, dev_loss_temp \u001b[39m=\u001b[39m SGD(X_tr, Y_tr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=16'>17</a>\u001b[0m                                 W,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=17'>18</a>\u001b[0m                                 X_dev\u001b[39m=\u001b[39;49mX_dev, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=18'>19</a>\u001b[0m                                 Y_dev\u001b[39m=\u001b[39;49mY_dev,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=19'>20</a>\u001b[0m                                 lr\u001b[39m=\u001b[39;49meta, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=20'>21</a>\u001b[0m                                 dropout\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=21'>22</a>\u001b[0m                                 freeze_emb\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=22'>23</a>\u001b[0m                                 tolerance\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=23'>24</a>\u001b[0m                                 epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=24'>25</a>\u001b[0m     final_dev_loss_2d[i][j] \u001b[39m=\u001b[39m dev_loss_temp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000050?line=25'>26</a>\u001b[0m     \u001b[39mif\u001b[39;00m dev_loss_temp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m min_loss:\n",
      "\u001b[1;32mc:\\Users\\jacob\\OneDrive\\Documents\\SHEFFIELD_2022\\NLP\\assignment.ipynb Cell 48'\u001b[0m in \u001b[0;36mSGD\u001b[1;34m(X_tr, Y_tr, W, X_dev, Y_dev, lr, dropout, epochs, tolerance, freeze_emb, print_progress)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000047?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch :\u001b[39m\u001b[39m'\u001b[39m, epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000047?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(X_tr): \u001b[39m# Training\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000047?line=19'>20</a>\u001b[0m     out_vals \u001b[39m=\u001b[39m forward_pass(data, weights, dropout_rate\u001b[39m=\u001b[39mdropout) \u001b[39m# Get output vals\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000047?line=20'>21</a>\u001b[0m     weights \u001b[39m=\u001b[39m backward_pass(data, Y_tr[i], weights, out_vals, lr\u001b[39m=\u001b[39mlr, freeze_emb\u001b[39m=\u001b[39mfreeze_emb) \u001b[39m# Update weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Documents/SHEFFIELD_2022/NLP/assignment.ipynb#ch0000047?line=21'>22</a>\u001b[0m loss_total \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'weights' referenced before assignment"
     ]
    }
   ],
   "source": [
    "loss_tr = []\n",
    "dev_loss = []\n",
    "\n",
    "W = network_weights(vocab_size=len(corp_vocabs),embedding_dim=300,hidden_dim=[], num_classes=3)\n",
    "W_final = W.copy()\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "min_loss = 99999 # dummy large number to test dev loss against\n",
    "# Automatic parse through potential values to find best\n",
    "learning_rate_values = np.round(np.logspace(-3, -1, 3),decimals=3) #Generated potential learning rate parameters\n",
    "dropout_rate_values = np.round(np.linspace(0.1, 0.5, 3),decimals=3) #Generated potential dropout rate parameters\n",
    "final_dev_loss_2d=np.zeros((len(learning_rate_values), len(dropout_rate_values)))\n",
    "for i, eta in enumerate(learning_rate_values):\n",
    "    for j, drop in enumerate(dropout_rate_values):\n",
    "        print (f'Trying params: LR {eta}, drop rate {drop}')\n",
    "        W_out, loss_tr_temp, dev_loss_temp = SGD(X_tr, Y_tr,\n",
    "                                    W,\n",
    "                                    X_dev=X_dev, \n",
    "                                    Y_dev=Y_dev,\n",
    "                                    lr=eta, \n",
    "                                    dropout=0.2,\n",
    "                                    freeze_emb=False,\n",
    "                                    tolerance=0.01,\n",
    "                                    epochs=20)\n",
    "        final_dev_loss_2d[i][j] = dev_loss_temp[-1]\n",
    "        if dev_loss_temp[-1] < min_loss:\n",
    "            min_loss = dev_loss_temp[-1]\n",
    "            W_final = W_out\n",
    "            loss_tr = loss_tr_temp\n",
    "            dev_loss = dev_loss_temp\n",
    "            print (f'New minimum found: LR {eta}, drop rate {drop}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:15.716497Z",
     "start_time": "2020-04-02T14:27:15.612736Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "plt.plot(range(len(loss_tr)), loss_tr,'-b', label='Train')\n",
    "plt.plot(range(len(dev_loss)), dev_loss,'-r', label='Dev')\n",
    "plt.legend()\n",
    "plt.xticks(range(len(loss_tr)))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train/Dev Loss for best params')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3244444444444444\n",
      "Precision: 0.31403998861902177\n",
      "Recall: 0.3244444444444444\n",
      "F1-Score: 0.30185109876813887\n"
     ]
    }
   ],
   "source": [
    "test_random = np.random.permutation(len(X_test))\n",
    "X_test = [X_test[i] for i in test_random]\n",
    "Y_test = [Y_test[i] for i in test_random]\n",
    "\n",
    "preds_test = [np.argmax(forward_pass(x, W_final, dropout_rate=0.0)['y'])+1 for x,y in zip(X_test,Y_test)]\n",
    "print('Accuracy:', accuracy_score(Y_test,preds_test))\n",
    "print('Precision:', precision_score(Y_test,preds_test,average='macro'))\n",
    "print('Recall:', recall_score(Y_test,preds_test,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_test,preds_test,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph is shown below of the average dev set loss with different several dropout rate and learning rate values, trained for 20 epochs with a embedding layer size of 50.\n",
    "\n",
    "- While more epochs does give very good results (the dropout layer prevents too much overfitting on test data), 20 was chosen as a good number due to the potentially large runtime of training the model.\n",
    "- Similarly, larger embedding layer size tended to give more favorable results, but to choose hyperparameters via fast testing the layer size 50 was used for all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "plt.imshow(final_dev_loss_2d,cmap=\"inferno\")\n",
    "plt.title(\"Best avg. dev loss for embedding layer size 50\")\n",
    "plt.xticks(np.arange(len(dropout_rate_values)), labels=dropout_rate_values)\n",
    "plt.yticks(np.arange(len(learning_rate_values)), labels=learning_rate_values)\n",
    "plt.xlabel('Dropout rate')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding matrix for your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    \n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                if word in corp_vocabs:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:28:54.548613Z",
     "start_time": "2020-04-02T14:27:32.780248Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\", {corp_vocabs[i]:i for i in range(len(corp_vocabs))}, emb_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:30:11.121198Z",
     "start_time": "2020-04-02T14:29:24.946124Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (6238, 50)\n",
      "Shape W1 (50, 3)\n",
      "Replacing embedding layer matrix with pretrained matrix of size (6238, 300)...\n",
      "Shape W0 (6238, 50)\n",
      "Shape W1 (50, 3)\n",
      "Epoch : 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8180/475245257.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mfinal_dev_loss_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropout_rate_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m W_out, loss_tr_temp, dev_loss_temp = SGD(X_tr, Y_tr,\n\u001b[0m\u001b[0;32m     20\u001b[0m                             \u001b[0mW_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                             \u001b[0mX_dev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8180/1013411880.py\u001b[0m in \u001b[0;36mSGD\u001b[1;34m(X_tr, Y_tr, W, X_dev, Y_dev, lr, dropout, epochs, tolerance, freeze_emb, print_progress)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mout_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Get output vals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreeze_emb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze_emb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mloss_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Testing on Train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8180/737993680.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[1;34m(x, y, W, out_vals, lr, freeze_emb)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0minput_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Initialise the one-hot input vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0minput_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# Make indices that are present 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error_deriv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;31m# Update embedding layer weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error_deriv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;31m# Update hidden layer weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(corp_vocabs),embedding_dim=50,hidden_dim=[], num_classes=3)\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "W[0] = w_glove # replace with pretrained weights\n",
    "print(f'Replacing embedding layer matrix with pretrained matrix of size {w_glove.shape}...')\n",
    "min_loss = 99999 # dummy large number to test dev loss against\n",
    "# Automatic parse through potential values to find best\n",
    "\n",
    "W_final = W.copy()\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "min_loss = 99999 # dummy large number to test dev loss against\n",
    "# Automatic parse through potential values to find best\n",
    "learning_rate_values = np.round(np.logspace(-3, -1, 3),decimals=3) #Generated potential learning rate parameters\n",
    "dropout_rate_values = np.round(np.linspace(0.1, 0.5, 3),decimals=3) #Generated potential dropout rate parameters\n",
    "final_dev_loss_2d=np.zeros((len(learning_rate_values), len(dropout_rate_values)))\n",
    "for i, eta in enumerate(learning_rate_values):\n",
    "    for j, drop in enumerate(dropout_rate_values):\n",
    "        W_out = W.copy()\n",
    "        print (f'Trying params: LR {eta}, drop rate {drop}')\n",
    "        W_out_temp, loss_tr_temp, dev_loss_temp = SGD(X_tr, Y_tr,\n",
    "                                    W_out,\n",
    "                                    X_dev=X_dev, \n",
    "                                    Y_dev=Y_dev,\n",
    "                                    lr=eta, \n",
    "                                    dropout=0.2,\n",
    "                                    freeze_emb=False,\n",
    "                                    tolerance=0.01,\n",
    "                                    epochs=20)\n",
    "        final_dev_loss_2d[i][j] = dev_loss_temp[-1]\n",
    "        if dev_loss_temp[-1] < min_loss:\n",
    "            min_loss = dev_loss_temp[-1]\n",
    "            W_final = W_out_temp.copy()\n",
    "            loss_tr = loss_tr_temp\n",
    "            dev_loss = dev_loss_temp\n",
    "            print (f'New minimum found: LR {eta}, drop rate {drop}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:12:00.815184Z",
     "start_time": "2020-04-02T15:12:00.812563Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_random = np.random.permutation(len(X_test))\n",
    "X_test = [X_test[i] for i in test_random]\n",
    "Y_test = [Y_test[i] for i in test_random]\n",
    "\n",
    "preds_test = [np.argmax(forward_pass(x, W_final, dropout_rate=0.0)['y'])+1 for x,y in zip(X_test,Y_test)]\n",
    "print('Accuracy:', accuracy_score(Y_test,preds_test))\n",
    "print('Precision:', precision_score(Y_test,preds_test,average='macro'))\n",
    "print('Recall:', recall_score(Y_test,preds_test,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_test,preds_test,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "plt.imshow(final_dev_loss_2d,cmap=\"inferno\")\n",
    "plt.title(\"Best avg. dev loss for embedding layer size 50\")\n",
    "plt.xticks(np.arange(len(dropout_rate_values)), labels=dropout_rate_values)\n",
    "plt.yticks(np.arange(len(learning_rate_values)), labels=learning_rate_values)\n",
    "plt.xlabel('Dropout rate')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures (Bonus)\n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = network_weights(vocab_size=len(corp_vocabs),embedding_dim=100,hidden_dim=[50], num_classes=3)\n",
    "W_final = W.copy()\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "min_loss = 99999 # dummy large number to test dev loss against\n",
    "# Automatic parse through potential values to find best\n",
    "learning_rate_values = np.round(np.logspace(-3, -1, 3),decimals=3) #Generated potential learning rate parameters\n",
    "dropout_rate_values = np.round(np.linspace(0.1, 0.5, 3),decimals=3) #Generated potential dropout rate parameters\n",
    "final_dev_loss_2d=np.zeros((len(learning_rate_values), len(dropout_rate_values)))\n",
    "for i, eta in enumerate(learning_rate_values):\n",
    "    for j, drop in enumerate(dropout_rate_values):\n",
    "        W_out = W.copy()\n",
    "        print (f'Trying params: LR {eta}, drop rate {drop}')\n",
    "        W_out_temp, loss_tr_temp, dev_loss_temp = SGD(X_tr, Y_tr,\n",
    "                                    W_out,\n",
    "                                    X_dev=X_dev, \n",
    "                                    Y_dev=Y_dev,\n",
    "                                    lr=eta, \n",
    "                                    dropout=0.2,\n",
    "                                    freeze_emb=False,\n",
    "                                    tolerance=0.01,\n",
    "                                    epochs=20)\n",
    "        final_dev_loss_2d[i][j] = dev_loss_temp[-1]\n",
    "        if dev_loss_temp[-1] < min_loss:\n",
    "            min_loss = dev_loss_temp[-1]\n",
    "            W_final = W_out_temp.copy()\n",
    "            loss_tr = loss_tr_temp\n",
    "            dev_loss = dev_loss_temp\n",
    "            print (f'New minimum found: LR {eta}, drop rate {drop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:11:51.994986Z",
     "start_time": "2020-04-02T15:11:51.992563Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) for x,y in zip(X_test,Y_test)]\n",
    "print('Accuracy:', accuracy_score(Y_test,preds_te))\n",
    "print('Precision:', precision_score(Y_test,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_test,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_test,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  |   |   |   |   |\n",
    "| Average Embedding (Pre-trained)  |   |   |   |   |\n",
    "| Average Embedding (Pre-trained) + X hidden layers (BONUS)   |   |   |   |   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
